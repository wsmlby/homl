# This Dockerfile builds the final HoML Server image for NVIDIA CUDA.
# It layers our application code on top of the official vLLM OpenAI image.

# Base image for CUDA builds
# vllm/vllm-openai:nightly-x86_64 @ 12/03/2025
FROM vllm/vllm-openai@sha256:4031092cf94d81404fa6453659113a5b75e28db41e4fa368f3037ecba254074b

# Set the working directory
WORKDIR /app


# Copy requirements.txt and install dependencies
COPY requirements.txt ./
RUN pip install -r requirements.txt


ENV ACCELERATOR=CUDA
# Copy our application source code
COPY ./homl_server ./homl_server
COPY ./vllm_patches ./patches

RUN cd /usr/local/lib/python3.12/dist-packages/vllm && patch -p1 < /app/patches/registry.patch

WORKDIR /app/homl_server
ARG HOML_SERVER_VERSION=dev
ENV HOML_SERVER_VERSION=$HOML_SERVER_VERSION

ENTRYPOINT [ "python3", "-u", "main.py"]
