{"traceEvents": [{"ph": "M", "pid": 2792750, "tid": 2792750, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 2792750, "tid": 2792750, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132371.355, "ph": "X", "cat": "fee", "dur": 5258063.268, "name": "builtins.__import__"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132371.23, "ph": "X", "cat": "fee", "dur": 5258063.663, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132370.423, "ph": "X", "cat": "fee", "dur": 5258317.508, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132359.664, "ph": "X", "cat": "fee", "dur": 5258334.379, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132358.338, "ph": "X", "cat": "fee", "dur": 5258336.607, "name": "builtins.__import__"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132358.085, "ph": "X", "cat": "fee", "dur": 5258337.039, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132356.542, "ph": "X", "cat": "fee", "dur": 5258552.648, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132332.133, "ph": "X", "cat": "fee", "dur": 5258583.126, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132329.222, "ph": "X", "cat": "fee", "dur": 5258587.391, "name": "builtins.__import__"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132285.223, "ph": "X", "cat": "fee", "dur": 5266141.103, "name": "_get_module_details (<frozen runpy>:105)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506811.741, "ph": "X", "cat": "fee", "dur": 522390.038, "name": "builtins.exec"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506811.551, "ph": "X", "cat": "fee", "dur": 522390.482, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506621.386, "ph": "X", "cat": "fee", "dur": 522580.915, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506606.901, "ph": "X", "cat": "fee", "dur": 522598.178, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506501.515, "ph": "X", "cat": "fee", "dur": 522703.833, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678506487.938, "ph": "X", "cat": "fee", "dur": 522723.352, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679057973.725, "ph": "X", "cat": "fee", "dur": 1355875.786, "name": "builtins.exec"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679057973.326, "ph": "X", "cat": "fee", "dur": 1355876.647, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679038319.542, "ph": "X", "cat": "fee", "dur": 1375530.709, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679038304.619, "ph": "X", "cat": "fee", "dur": 1375546.501, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679038264.078, "ph": "X", "cat": "fee", "dur": 1375587.754, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918679038250.137, "ph": "X", "cat": "fee", "dur": 1375607.393, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678398469.527, "ph": "X", "cat": "fee", "dur": 2015389.244, "name": "<module> (/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py:1)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678398455.847, "ph": "X", "cat": "fee", "dur": 2015403.375, "name": "builtins.exec"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678398442.918, "ph": "X", "cat": "fee", "dur": 2015416.424, "name": "_run_code (<frozen runpy>:65)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918678398429.317, "ph": "X", "cat": "fee", "dur": 2015436.864, "name": "_run_module_code (<frozen runpy>:91)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132282.02, "ph": "X", "cat": "fee", "dur": 7281584.473, "name": "run_module (<frozen runpy>:201)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132280.65, "ph": "X", "cat": "fee", "dur": 7281586.16, "name": "<module> (<string>:1)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918673132235.313, "ph": "X", "cat": "fee", "dur": 7281633.574, "name": "builtins.exec"}, {"pid": 2792750, "tid": 2792750, "ts": 1918680418811.094, "ph": "X", "cat": "fee", "dur": 31.799, "name": "Finalize.__call__ (/usr/lib/python3.12/multiprocessing/util.py:208)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918680418805.93, "ph": "X", "cat": "fee", "dur": 37.053, "name": "_run_finalizers (/usr/lib/python3.12/multiprocessing/util.py:271)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918680418792.183, "ph": "X", "cat": "fee", "dur": 50.825, "name": "_exit_function (/usr/lib/python3.12/multiprocessing/util.py:323)"}, {"pid": 2792750, "tid": 2792750, "ts": 1918680413871.147, "ph": "X", "cat": "fee", "dur": 4971.867, "name": "atexit._run_exitfuncs"}], "viztracer_metadata": {"overflow": false, "version": "1.0.4"}, "file_info": {"files": {"<frozen importlib._bootstrap>": ["\"\"\"Core implementation of import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n#\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module\n# in the early stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\ndef _object_name(obj):\n    try:\n        return obj.__qualname__\n    except AttributeError:\n        return type(obj).__qualname__\n\n# Bootstrap-related code ######################################################\n\n# Modules injected manually by _setup()\n_thread = None\n_warnings = None\n_weakref = None\n\n# Import done by _install_external_importers()\n_bootstrap_external = None\n\n\ndef _wrap(new, old):\n    \"\"\"Simple substitute for functools.update_wrapper.\"\"\"\n    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n        if hasattr(old, replace):\n            setattr(new, replace, getattr(old, replace))\n    new.__dict__.update(old.__dict__)\n\n\ndef _new_module(name):\n    return type(sys)(name)\n\n\n# Module-level locking ########################################################\n\n# For a list that can have a weakref to it.\nclass _List(list):\n    pass\n\n\n# Copied from weakref.py with some simplifications and modifications unique to\n# bootstrapping importlib. Many methods were simply deleting for simplicity, so if they\n# are needed in the future they may work if simply copied back in.\nclass _WeakValueDictionary:\n\n    def __init__(self):\n        self_weakref = _weakref.ref(self)\n\n        # Inlined to avoid issues with inheriting from _weakref.ref before _weakref is\n        # set by _setup(). Since there's only one instance of this class, this is\n        # not expensive.\n        class KeyedRef(_weakref.ref):\n\n            __slots__ = \"key\",\n\n            def __new__(type, ob, key):\n                self = super().__new__(type, ob, type.remove)\n                self.key = key\n                return self\n\n            def __init__(self, ob, key):\n                super().__init__(ob, self.remove)\n\n            @staticmethod\n            def remove(wr):\n                nonlocal self_weakref\n\n                self = self_weakref()\n                if self is not None:\n                    if self._iterating:\n                        self._pending_removals.append(wr.key)\n                    else:\n                        _weakref._remove_dead_weakref(self.data, wr.key)\n\n        self._KeyedRef = KeyedRef\n        self.clear()\n\n    def clear(self):\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _weakref._remove_dead_weakref(d, key)\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            if (o := wr()) is None:\n                return default\n            else:\n                return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = self._KeyedRef(default, key)\n            return default\n        else:\n            return o\n\n\n# A dict mapping module names to weakrefs of _ModuleLock instances.\n# Dictionary protected by the global import lock.\n_module_locks = {}\n\n# A dict mapping thread IDs to weakref'ed lists of _ModuleLock instances.\n# This maps a thread to the module locks it is blocking on acquiring.  The\n# values are lists because a single thread could perform a re-entrant import\n# and be \"in the process\" of blocking on locks for more than one module.  A\n# thread can be \"in the process\" because a thread cannot actually block on\n# acquiring more than one lock but it can have set up bookkeeping that reflects\n# that it intends to block on acquiring more than one lock.\n#\n# The dictionary uses a WeakValueDictionary to avoid keeping unnecessary\n# lists around, regardless of GC runs. This way there's no memory leak if\n# the list is no longer needed (GH-106176).\n_blocking_on = None\n\n\nclass _BlockingOnManager:\n    \"\"\"A context manager responsible to updating ``_blocking_on``.\"\"\"\n    def __init__(self, thread_id, lock):\n        self.thread_id = thread_id\n        self.lock = lock\n\n    def __enter__(self):\n        \"\"\"Mark the running thread as waiting for self.lock. via _blocking_on.\"\"\"\n        # Interactions with _blocking_on are *not* protected by the global\n        # import lock here because each thread only touches the state that it\n        # owns (state keyed on its thread id).  The global import lock is\n        # re-entrant (i.e., a single thread may take it more than once) so it\n        # wouldn't help us be correct in the face of re-entrancy either.\n\n        self.blocked_on = _blocking_on.setdefault(self.thread_id, _List())\n        self.blocked_on.append(self.lock)\n\n    def __exit__(self, *args, **kwargs):\n        \"\"\"Remove self.lock from this thread's _blocking_on list.\"\"\"\n        self.blocked_on.remove(self.lock)\n\n\nclass _DeadlockError(RuntimeError):\n    pass\n\n\n\ndef _has_deadlocked(target_id, *, seen_ids, candidate_ids, blocking_on):\n    \"\"\"Check if 'target_id' is holding the same lock as another thread(s).\n\n    The search within 'blocking_on' starts with the threads listed in\n    'candidate_ids'.  'seen_ids' contains any threads that are considered\n    already traversed in the search.\n\n    Keyword arguments:\n    target_id     -- The thread id to try to reach.\n    seen_ids      -- A set of threads that have already been visited.\n    candidate_ids -- The thread ids from which to begin.\n    blocking_on   -- A dict representing the thread/blocking-on graph.  This may\n                     be the same object as the global '_blocking_on' but it is\n                     a parameter to reduce the impact that global mutable\n                     state has on the result of this function.\n    \"\"\"\n    if target_id in candidate_ids:\n        # If we have already reached the target_id, we're done - signal that it\n        # is reachable.\n        return True\n\n    # Otherwise, try to reach the target_id from each of the given candidate_ids.\n    for tid in candidate_ids:\n        if not (candidate_blocking_on := blocking_on.get(tid)):\n            # There are no edges out from this node, skip it.\n            continue\n        elif tid in seen_ids:\n            # bpo 38091: the chain of tid's we encounter here eventually leads\n            # to a fixed point or a cycle, but does not reach target_id.\n            # This means we would not actually deadlock.  This can happen if\n            # other threads are at the beginning of acquire() below.\n            return False\n        seen_ids.add(tid)\n\n        # Follow the edges out from this thread.\n        edges = [lock.owner for lock in candidate_blocking_on]\n        if _has_deadlocked(target_id, seen_ids=seen_ids, candidate_ids=edges,\n                blocking_on=blocking_on):\n            return True\n\n    return False\n\n\nclass _ModuleLock:\n    \"\"\"A recursive lock implementation which is able to detect deadlocks\n    (e.g. thread 1 trying to take locks A then B, and thread 2 trying to\n    take locks B then A).\n    \"\"\"\n\n    def __init__(self, name):\n        # Create an RLock for protecting the import process for the\n        # corresponding module.  Since it is an RLock, a single thread will be\n        # able to take it more than once.  This is necessary to support\n        # re-entrancy in the import system that arises from (at least) signal\n        # handlers and the garbage collector.  Consider the case of:\n        #\n        #  import foo\n        #  -> ...\n        #     -> importlib._bootstrap._ModuleLock.acquire\n        #        -> ...\n        #           -> <garbage collector>\n        #              -> __del__\n        #                 -> import foo\n        #                    -> ...\n        #                       -> importlib._bootstrap._ModuleLock.acquire\n        #                          -> _BlockingOnManager.__enter__\n        #\n        # If a different thread than the running one holds the lock then the\n        # thread will have to block on taking the lock, which is what we want\n        # for thread safety.\n        self.lock = _thread.RLock()\n        self.wakeup = _thread.allocate_lock()\n\n        # The name of the module for which this is a lock.\n        self.name = name\n\n        # Can end up being set to None if this lock is not owned by any thread\n        # or the thread identifier for the owning thread.\n        self.owner = None\n\n        # Represent the number of times the owning thread has acquired this lock\n        # via a list of True.  This supports RLock-like (\"re-entrant lock\")\n        # behavior, necessary in case a single thread is following a circular\n        # import dependency and needs to take the lock for a single module\n        # more than once.\n        #\n        # Counts are represented as a list of True because list.append(True)\n        # and list.pop() are both atomic and thread-safe in CPython and it's hard\n        # to find another primitive with the same properties.\n        self.count = []\n\n        # This is a count of the number of threads that are blocking on\n        # self.wakeup.acquire() awaiting to get their turn holding this module\n        # lock.  When the module lock is released, if this is greater than\n        # zero, it is decremented and `self.wakeup` is released one time.  The\n        # intent is that this will let one other thread make more progress on\n        # acquiring this module lock.  This repeats until all the threads have\n        # gotten a turn.\n        #\n        # This is incremented in self.acquire() when a thread notices it is\n        # going to have to wait for another thread to finish.\n        #\n        # See the comment above count for explanation of the representation.\n        self.waiters = []\n\n    def has_deadlock(self):\n        # To avoid deadlocks for concurrent or re-entrant circular imports,\n        # look at _blocking_on to see if any threads are blocking\n        # on getting the import lock for any module for which the import lock\n        # is held by this thread.\n        return _has_deadlocked(\n            # Try to find this thread.\n            target_id=_thread.get_ident(),\n            seen_ids=set(),\n            # Start from the thread that holds the import lock for this\n            # module.\n            candidate_ids=[self.owner],\n            # Use the global \"blocking on\" state.\n            blocking_on=_blocking_on,\n        )\n\n    def acquire(self):\n        \"\"\"\n        Acquire the module lock.  If a potential deadlock is detected,\n        a _DeadlockError is raised.\n        Otherwise, the lock is always acquired and True is returned.\n        \"\"\"\n        tid = _thread.get_ident()\n        with _BlockingOnManager(tid, self):\n            while True:\n                # Protect interaction with state on self with a per-module\n                # lock.  This makes it safe for more than one thread to try to\n                # acquire the lock for a single module at the same time.\n                with self.lock:\n                    if self.count == [] or self.owner == tid:\n                        # If the lock for this module is unowned then we can\n                        # take the lock immediately and succeed.  If the lock\n                        # for this module is owned by the running thread then\n                        # we can also allow the acquire to succeed.  This\n                        # supports circular imports (thread T imports module A\n                        # which imports module B which imports module A).\n                        self.owner = tid\n                        self.count.append(True)\n                        return True\n\n                    # At this point we know the lock is held (because count !=\n                    # 0) by another thread (because owner != tid).  We'll have\n                    # to get in line to take the module lock.\n\n                    # But first, check to see if this thread would create a\n                    # deadlock by acquiring this module lock.  If it would\n                    # then just stop with an error.\n                    #\n                    # It's not clear who is expected to handle this error.\n                    # There is one handler in _lock_unlock_module but many\n                    # times this method is called when entering the context\n                    # manager _ModuleLockManager instead - so _DeadlockError\n                    # will just propagate up to application code.\n                    #\n                    # This seems to be more than just a hypothetical -\n                    # https://stackoverflow.com/questions/59509154\n                    # https://github.com/encode/django-rest-framework/issues/7078\n                    if self.has_deadlock():\n                        raise _DeadlockError(f'deadlock detected by {self!r}')\n\n                    # Check to see if we're going to be able to acquire the\n                    # lock.  If we are going to have to wait then increment\n                    # the waiters so `self.release` will know to unblock us\n                    # later on.  We do this part non-blockingly so we don't\n                    # get stuck here before we increment waiters.  We have\n                    # this extra acquire call (in addition to the one below,\n                    # outside the self.lock context manager) to make sure\n                    # self.wakeup is held when the next acquire is called (so\n                    # we block).  This is probably needlessly complex and we\n                    # should just take self.wakeup in the return codepath\n                    # above.\n                    if self.wakeup.acquire(False):\n                        self.waiters.append(None)\n\n                # Now take the lock in a blocking fashion.  This won't\n                # complete until the thread holding this lock\n                # (self.owner) calls self.release.\n                self.wakeup.acquire()\n\n                # Taking the lock has served its purpose (making us wait), so we can\n                # give it up now.  We'll take it w/o blocking again on the\n                # next iteration around this 'while' loop.\n                self.wakeup.release()\n\n    def release(self):\n        tid = _thread.get_ident()\n        with self.lock:\n            if self.owner != tid:\n                raise RuntimeError('cannot release un-acquired lock')\n            assert len(self.count) > 0\n            self.count.pop()\n            if not len(self.count):\n                self.owner = None\n                if len(self.waiters) > 0:\n                    self.waiters.pop()\n                    self.wakeup.release()\n\n    def __repr__(self):\n        return f'_ModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _DummyModuleLock:\n    \"\"\"A simple _ModuleLock equivalent for Python builds without\n    multi-threading support.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        self.count = 0\n\n    def acquire(self):\n        self.count += 1\n        return True\n\n    def release(self):\n        if self.count == 0:\n            raise RuntimeError('cannot release un-acquired lock')\n        self.count -= 1\n\n    def __repr__(self):\n        return f'_DummyModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _ModuleLockManager:\n\n    def __init__(self, name):\n        self._name = name\n        self._lock = None\n\n    def __enter__(self):\n        self._lock = _get_module_lock(self._name)\n        self._lock.acquire()\n\n    def __exit__(self, *args, **kwargs):\n        self._lock.release()\n\n\n# The following two functions are for consumption by Python/import.c.\n\ndef _get_module_lock(name):\n    \"\"\"Get or create the module lock for a given module name.\n\n    Acquire/release internally the global import lock to protect\n    _module_locks.\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        try:\n            lock = _module_locks[name]()\n        except KeyError:\n            lock = None\n\n        if lock is None:\n            if _thread is None:\n                lock = _DummyModuleLock(name)\n            else:\n                lock = _ModuleLock(name)\n\n            def cb(ref, name=name):\n                _imp.acquire_lock()\n                try:\n                    # bpo-31070: Check if another thread created a new lock\n                    # after the previous lock was destroyed\n                    # but before the weakref callback was called.\n                    if _module_locks.get(name) is ref:\n                        del _module_locks[name]\n                finally:\n                    _imp.release_lock()\n\n            _module_locks[name] = _weakref.ref(lock, cb)\n    finally:\n        _imp.release_lock()\n\n    return lock\n\n\ndef _lock_unlock_module(name):\n    \"\"\"Acquires then releases the module lock for a given module name.\n\n    This is used to ensure a module is completely initialized, in the\n    event it is being imported by another thread.\n    \"\"\"\n    lock = _get_module_lock(name)\n    try:\n        lock.acquire()\n    except _DeadlockError:\n        # Concurrent circular import, we'll accept a partially initialized\n        # module object.\n        pass\n    else:\n        lock.release()\n\n# Frame stripping magic ###############################################\ndef _call_with_frames_removed(f, *args, **kwds):\n    \"\"\"remove_importlib_frames in import.c will always remove sequences\n    of importlib frames that end with a call to this function\n\n    Use it instead of a normal call in places where including the importlib\n    frames introduces unwanted noise into the traceback (e.g. when executing\n    module code)\n    \"\"\"\n    return f(*args, **kwds)\n\n\ndef _verbose_message(message, *args, verbosity=1):\n    \"\"\"Print the message to stderr if -v/PYTHONVERBOSE is turned on.\"\"\"\n    if sys.flags.verbose >= verbosity:\n        if not message.startswith(('#', 'import ')):\n            message = '# ' + message\n        print(message.format(*args), file=sys.stderr)\n\n\ndef _requires_builtin(fxn):\n    \"\"\"Decorator to verify the named module is built-in.\"\"\"\n    def _requires_builtin_wrapper(self, fullname):\n        if fullname not in sys.builtin_module_names:\n            raise ImportError(f'{fullname!r} is not a built-in module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_builtin_wrapper, fxn)\n    return _requires_builtin_wrapper\n\n\ndef _requires_frozen(fxn):\n    \"\"\"Decorator to verify the named module is frozen.\"\"\"\n    def _requires_frozen_wrapper(self, fullname):\n        if not _imp.is_frozen(fullname):\n            raise ImportError(f'{fullname!r} is not a frozen module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_frozen_wrapper, fxn)\n    return _requires_frozen_wrapper\n\n\n# Typically used by loader classes as a method replacement.\ndef _load_module_shim(self, fullname):\n    \"\"\"Load the specified module into sys.modules and return it.\n\n    This method is deprecated.  Use loader.exec_module() instead.\n\n    \"\"\"\n    msg = (\"the load_module() method is deprecated and slated for removal in \"\n           \"Python 3.15; use exec_module() instead\")\n    _warnings.warn(msg, DeprecationWarning)\n    spec = spec_from_loader(fullname, self)\n    if fullname in sys.modules:\n        module = sys.modules[fullname]\n        _exec(spec, module)\n        return sys.modules[fullname]\n    else:\n        return _load(spec)\n\n# Module specifications #######################################################\n\ndef _module_repr(module):\n    \"\"\"The implementation of ModuleType.__repr__().\"\"\"\n    loader = getattr(module, '__loader__', None)\n    if spec := getattr(module, \"__spec__\", None):\n        return _module_repr_from_spec(spec)\n    # Fall through to a catch-all which always succeeds.\n    try:\n        name = module.__name__\n    except AttributeError:\n        name = '?'\n    try:\n        filename = module.__file__\n    except AttributeError:\n        if loader is None:\n            return f'<module {name!r}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        return f'<module {name!r} from {filename!r}>'\n\n\nclass ModuleSpec:\n    \"\"\"The specification for a module, used for loading.\n\n    A module's spec is the source for information about the module.  For\n    data associated with the module, including source, use the spec's\n    loader.\n\n    `name` is the absolute name of the module.  `loader` is the loader\n    to use when loading the module.  `parent` is the name of the\n    package the module is in.  The parent is derived from the name.\n\n    `is_package` determines if the module is considered a package or\n    not.  On modules this is reflected by the `__path__` attribute.\n\n    `origin` is the specific location used by the loader from which to\n    load the module, if that information is available.  When filename is\n    set, origin will match.\n\n    `has_location` indicates that a spec's \"origin\" reflects a location.\n    When this is True, `__file__` attribute of the module is set.\n\n    `cached` is the location of the cached bytecode file, if any.  It\n    corresponds to the `__cached__` attribute.\n\n    `submodule_search_locations` is the sequence of path entries to\n    search when importing submodules.  If set, is_package should be\n    True--and False otherwise.\n\n    Packages are simply modules that (may) have submodules.  If a spec\n    has a non-None value in `submodule_search_locations`, the import\n    system will consider modules loaded from the spec as packages.\n\n    Only finders (see importlib.abc.MetaPathFinder and\n    importlib.abc.PathEntryFinder) should modify ModuleSpec instances.\n\n    \"\"\"\n\n    def __init__(self, name, loader, *, origin=None, loader_state=None,\n                 is_package=None):\n        self.name = name\n        self.loader = loader\n        self.origin = origin\n        self.loader_state = loader_state\n        self.submodule_search_locations = [] if is_package else None\n        self._uninitialized_submodules = []\n\n        # file-location attributes\n        self._set_fileattr = False\n        self._cached = None\n\n    def __repr__(self):\n        args = [f'name={self.name!r}', f'loader={self.loader!r}']\n        if self.origin is not None:\n            args.append(f'origin={self.origin!r}')\n        if self.submodule_search_locations is not None:\n            args.append(f'submodule_search_locations={self.submodule_search_locations}')\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __eq__(self, other):\n        smsl = self.submodule_search_locations\n        try:\n            return (self.name == other.name and\n                    self.loader == other.loader and\n                    self.origin == other.origin and\n                    smsl == other.submodule_search_locations and\n                    self.cached == other.cached and\n                    self.has_location == other.has_location)\n        except AttributeError:\n            return NotImplemented\n\n    @property\n    def cached(self):\n        if self._cached is None:\n            if self.origin is not None and self._set_fileattr:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                self._cached = _bootstrap_external._get_cached(self.origin)\n        return self._cached\n\n    @cached.setter\n    def cached(self, cached):\n        self._cached = cached\n\n    @property\n    def parent(self):\n        \"\"\"The name of the module's parent.\"\"\"\n        if self.submodule_search_locations is None:\n            return self.name.rpartition('.')[0]\n        else:\n            return self.name\n\n    @property\n    def has_location(self):\n        return self._set_fileattr\n\n    @has_location.setter\n    def has_location(self, value):\n        self._set_fileattr = bool(value)\n\n\ndef spec_from_loader(name, loader, *, origin=None, is_package=None):\n    \"\"\"Return a module spec based on various loader methods.\"\"\"\n    if origin is None:\n        origin = getattr(loader, '_ORIGIN', None)\n\n    if not origin and hasattr(loader, 'get_filename'):\n        if _bootstrap_external is None:\n            raise NotImplementedError\n        spec_from_file_location = _bootstrap_external.spec_from_file_location\n\n        if is_package is None:\n            return spec_from_file_location(name, loader=loader)\n        search = [] if is_package else None\n        return spec_from_file_location(name, loader=loader,\n                                       submodule_search_locations=search)\n\n    if is_package is None:\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                is_package = None  # aka, undefined\n        else:\n            # the default\n            is_package = False\n\n    return ModuleSpec(name, loader, origin=origin, is_package=is_package)\n\n\ndef _spec_from_module(module, loader=None, origin=None):\n    # This function is meant for use in _setup().\n    try:\n        spec = module.__spec__\n    except AttributeError:\n        pass\n    else:\n        if spec is not None:\n            return spec\n\n    name = module.__name__\n    if loader is None:\n        try:\n            loader = module.__loader__\n        except AttributeError:\n            # loader will stay None.\n            pass\n    try:\n        location = module.__file__\n    except AttributeError:\n        location = None\n    if origin is None:\n        if loader is not None:\n            origin = getattr(loader, '_ORIGIN', None)\n        if not origin and location is not None:\n            origin = location\n    try:\n        cached = module.__cached__\n    except AttributeError:\n        cached = None\n    try:\n        submodule_search_locations = list(module.__path__)\n    except AttributeError:\n        submodule_search_locations = None\n\n    spec = ModuleSpec(name, loader, origin=origin)\n    spec._set_fileattr = False if location is None else (origin == location)\n    spec.cached = cached\n    spec.submodule_search_locations = submodule_search_locations\n    return spec\n\n\ndef _init_module_attrs(spec, module, *, override=False):\n    # The passed-in module may be not support attribute assignment,\n    # in which case we simply don't set the attributes.\n    # __name__\n    if (override or getattr(module, '__name__', None) is None):\n        try:\n            module.__name__ = spec.name\n        except AttributeError:\n            pass\n    # __loader__\n    if override or getattr(module, '__loader__', None) is None:\n        loader = spec.loader\n        if loader is None:\n            # A backward compatibility hack.\n            if spec.submodule_search_locations is not None:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                NamespaceLoader = _bootstrap_external.NamespaceLoader\n\n                loader = NamespaceLoader.__new__(NamespaceLoader)\n                loader._path = spec.submodule_search_locations\n                spec.loader = loader\n                # While the docs say that module.__file__ is not set for\n                # built-in modules, and the code below will avoid setting it if\n                # spec.has_location is false, this is incorrect for namespace\n                # packages.  Namespace packages have no location, but their\n                # __spec__.origin is None, and thus their module.__file__\n                # should also be None for consistency.  While a bit of a hack,\n                # this is the best place to ensure this consistency.\n                #\n                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module\n                # and bpo-32305\n                module.__file__ = None\n        try:\n            module.__loader__ = loader\n        except AttributeError:\n            pass\n    # __package__\n    if override or getattr(module, '__package__', None) is None:\n        try:\n            module.__package__ = spec.parent\n        except AttributeError:\n            pass\n    # __spec__\n    try:\n        module.__spec__ = spec\n    except AttributeError:\n        pass\n    # __path__\n    if override or getattr(module, '__path__', None) is None:\n        if spec.submodule_search_locations is not None:\n            # XXX We should extend __path__ if it's already a list.\n            try:\n                module.__path__ = spec.submodule_search_locations\n            except AttributeError:\n                pass\n    # __file__/__cached__\n    if spec.has_location:\n        if override or getattr(module, '__file__', None) is None:\n            try:\n                module.__file__ = spec.origin\n            except AttributeError:\n                pass\n\n        if override or getattr(module, '__cached__', None) is None:\n            if spec.cached is not None:\n                try:\n                    module.__cached__ = spec.cached\n                except AttributeError:\n                    pass\n    return module\n\n\ndef module_from_spec(spec):\n    \"\"\"Create a module based on the provided spec.\"\"\"\n    # Typically loaders will not implement create_module().\n    module = None\n    if hasattr(spec.loader, 'create_module'):\n        # If create_module() returns `None` then it means default\n        # module creation should be used.\n        module = spec.loader.create_module(spec)\n    elif hasattr(spec.loader, 'exec_module'):\n        raise ImportError('loaders that define exec_module() '\n                          'must also define create_module()')\n    if module is None:\n        module = _new_module(spec.name)\n    _init_module_attrs(spec, module)\n    return module\n\n\ndef _module_repr_from_spec(spec):\n    \"\"\"Return the repr to use for the module.\"\"\"\n    name = '?' if spec.name is None else spec.name\n    if spec.origin is None:\n        loader = spec.loader\n        if loader is None:\n            return f'<module {name!r}>'\n        elif (\n            _bootstrap_external is not None\n            and isinstance(loader, _bootstrap_external.NamespaceLoader)\n        ):\n            return f'<module {name!r} (namespace) from {list(loader._path)}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        if spec.has_location:\n            return f'<module {name!r} from {spec.origin!r}>'\n        else:\n            return f'<module {spec.name!r} ({spec.origin})>'\n\n\n# Used by importlib.reload() and _load_module_shim().\ndef _exec(spec, module):\n    \"\"\"Execute the spec's specified module in an existing module's namespace.\"\"\"\n    name = spec.name\n    with _ModuleLockManager(name):\n        if sys.modules.get(name) is not module:\n            msg = f'module {name!r} not in sys.modules'\n            raise ImportError(msg, name=name)\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # Namespace package.\n                _init_module_attrs(spec, module, override=True)\n            else:\n                _init_module_attrs(spec, module, override=True)\n                if not hasattr(spec.loader, 'exec_module'):\n                    msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                           \"falling back to load_module()\")\n                    _warnings.warn(msg, ImportWarning)\n                    spec.loader.load_module(name)\n                else:\n                    spec.loader.exec_module(module)\n        finally:\n            # Update the order of insertion into sys.modules for module\n            # clean-up at shutdown.\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n    return module\n\n\ndef _load_backward_compatible(spec):\n    # It is assumed that all callers have been warned about using load_module()\n    # appropriately before calling this function.\n    try:\n        spec.loader.load_module(spec.name)\n    except:\n        if spec.name in sys.modules:\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n        raise\n    # The module must be in sys.modules at this point!\n    # Move it to the end of sys.modules.\n    module = sys.modules.pop(spec.name)\n    sys.modules[spec.name] = module\n    if getattr(module, '__loader__', None) is None:\n        try:\n            module.__loader__ = spec.loader\n        except AttributeError:\n            pass\n    if getattr(module, '__package__', None) is None:\n        try:\n            # Since module.__path__ may not line up with\n            # spec.submodule_search_paths, we can't necessarily rely\n            # on spec.parent here.\n            module.__package__ = module.__name__\n            if not hasattr(module, '__path__'):\n                module.__package__ = spec.name.rpartition('.')[0]\n        except AttributeError:\n            pass\n    if getattr(module, '__spec__', None) is None:\n        try:\n            module.__spec__ = spec\n        except AttributeError:\n            pass\n    return module\n\ndef _load_unlocked(spec):\n    # A helper for direct use by the import system.\n    if spec.loader is not None:\n        # Not a namespace package.\n        if not hasattr(spec.loader, 'exec_module'):\n            msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                    \"falling back to load_module()\")\n            _warnings.warn(msg, ImportWarning)\n            return _load_backward_compatible(spec)\n\n    module = module_from_spec(spec)\n\n    # This must be done before putting the module in sys.modules\n    # (otherwise an optimization shortcut in import.c becomes\n    # wrong).\n    spec._initializing = True\n    try:\n        sys.modules[spec.name] = module\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # A namespace package so do nothing.\n            else:\n                spec.loader.exec_module(module)\n        except:\n            try:\n                del sys.modules[spec.name]\n            except KeyError:\n                pass\n            raise\n        # Move the module to the end of sys.modules.\n        # We don't ensure that the import-related module attributes get\n        # set in the sys.modules replacement case.  Such modules are on\n        # their own.\n        module = sys.modules.pop(spec.name)\n        sys.modules[spec.name] = module\n        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)\n    finally:\n        spec._initializing = False\n\n    return module\n\n# A method used during testing of _load_unlocked() and by\n# _load_module_shim().\ndef _load(spec):\n    \"\"\"Return a new module object, loaded by the spec's loader.\n\n    The module is not added to its parent.\n\n    If a module is already in sys.modules, that existing module gets\n    clobbered.\n\n    \"\"\"\n    with _ModuleLockManager(spec.name):\n        return _load_unlocked(spec)\n\n\n# Loaders #####################################################################\n\nclass BuiltinImporter:\n\n    \"\"\"Meta path import for built-in modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"built-in\"\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        if _imp.is_builtin(fullname):\n            return spec_from_loader(fullname, cls, origin=cls._ORIGIN)\n        else:\n            return None\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Create a built-in module\"\"\"\n        if spec.name not in sys.builtin_module_names:\n            raise ImportError(f'{spec.name!r} is not a built-in module',\n                              name=spec.name)\n        return _call_with_frames_removed(_imp.create_builtin, spec)\n\n    @staticmethod\n    def exec_module(module):\n        \"\"\"Exec a built-in module\"\"\"\n        _call_with_frames_removed(_imp.exec_builtin, module)\n\n    @classmethod\n    @_requires_builtin\n    def get_code(cls, fullname):\n        \"\"\"Return None as built-in modules do not have code objects.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def get_source(cls, fullname):\n        \"\"\"Return None as built-in modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def is_package(cls, fullname):\n        \"\"\"Return False as built-in modules are never packages.\"\"\"\n        return False\n\n    load_module = classmethod(_load_module_shim)\n\n\nclass FrozenImporter:\n\n    \"\"\"Meta path import for frozen modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"frozen\"\n\n    @classmethod\n    def _fix_up_module(cls, module):\n        spec = module.__spec__\n        state = spec.loader_state\n        if state is None:\n            # The module is missing FrozenImporter-specific values.\n\n            # Fix up the spec attrs.\n            origname = vars(module).pop('__origname__', None)\n            assert origname, 'see PyImport_ImportFrozenModuleObject()'\n            ispkg = hasattr(module, '__path__')\n            assert _imp.is_frozen_package(module.__name__) == ispkg, ispkg\n            filename, pkgdir = cls._resolve_filename(origname, spec.name, ispkg)\n            spec.loader_state = type(sys.implementation)(\n                filename=filename,\n                origname=origname,\n            )\n            __path__ = spec.submodule_search_locations\n            if ispkg:\n                assert __path__ == [], __path__\n                if pkgdir:\n                    spec.submodule_search_locations.insert(0, pkgdir)\n            else:\n                assert __path__ is None, __path__\n\n            # Fix up the module attrs (the bare minimum).\n            assert not hasattr(module, '__file__'), module.__file__\n            if filename:\n                try:\n                    module.__file__ = filename\n                except AttributeError:\n                    pass\n            if ispkg:\n                if module.__path__ != __path__:\n                    assert module.__path__ == [], module.__path__\n                    module.__path__.extend(__path__)\n        else:\n            # These checks ensure that _fix_up_module() is only called\n            # in the right places.\n            __path__ = spec.submodule_search_locations\n            ispkg = __path__ is not None\n            # Check the loader state.\n            assert sorted(vars(state)) == ['filename', 'origname'], state\n            if state.origname:\n                # The only frozen modules with \"origname\" set are stdlib modules.\n                (__file__, pkgdir,\n                 ) = cls._resolve_filename(state.origname, spec.name, ispkg)\n                assert state.filename == __file__, (state.filename, __file__)\n                if pkgdir:\n                    assert __path__ == [pkgdir], (__path__, pkgdir)\n                else:\n                    assert __path__ == ([] if ispkg else None), __path__\n            else:\n                __file__ = None\n                assert state.filename is None, state.filename\n                assert __path__ == ([] if ispkg else None), __path__\n            # Check the file attrs.\n            if __file__:\n                assert hasattr(module, '__file__')\n                assert module.__file__ == __file__, (module.__file__, __file__)\n            else:\n                assert not hasattr(module, '__file__'), module.__file__\n            if ispkg:\n                assert hasattr(module, '__path__')\n                assert module.__path__ == __path__, (module.__path__, __path__)\n            else:\n                assert not hasattr(module, '__path__'), module.__path__\n        assert not spec.has_location\n\n    @classmethod\n    def _resolve_filename(cls, fullname, alias=None, ispkg=False):\n        if not fullname or not getattr(sys, '_stdlib_dir', None):\n            return None, None\n        try:\n            sep = cls._SEP\n        except AttributeError:\n            sep = cls._SEP = '\\\\' if sys.platform == 'win32' else '/'\n\n        if fullname != alias:\n            if fullname.startswith('<'):\n                fullname = fullname[1:]\n                if not ispkg:\n                    fullname = f'{fullname}.__init__'\n            else:\n                ispkg = False\n        relfile = fullname.replace('.', sep)\n        if ispkg:\n            pkgdir = f'{sys._stdlib_dir}{sep}{relfile}'\n            filename = f'{pkgdir}{sep}__init__.py'\n        else:\n            pkgdir = None\n            filename = f'{sys._stdlib_dir}{sep}{relfile}.py'\n        return filename, pkgdir\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        info = _call_with_frames_removed(_imp.find_frozen, fullname)\n        if info is None:\n            return None\n        # We get the marshaled data in exec_module() (the loader\n        # part of the importer), instead of here (the finder part).\n        # The loader is the usual place to get the data that will\n        # be loaded into the module.  (For example, see _LoaderBasics\n        # in _bootstra_external.py.)  Most importantly, this importer\n        # is simpler if we wait to get the data.\n        # However, getting as much data in the finder as possible\n        # to later load the module is okay, and sometimes important.\n        # (That's why ModuleSpec.loader_state exists.)  This is\n        # especially true if it avoids throwing away expensive data\n        # the loader would otherwise duplicate later and can be done\n        # efficiently.  In this case it isn't worth it.\n        _, ispkg, origname = info\n        spec = spec_from_loader(fullname, cls,\n                                origin=cls._ORIGIN,\n                                is_package=ispkg)\n        filename, pkgdir = cls._resolve_filename(origname, fullname, ispkg)\n        spec.loader_state = type(sys.implementation)(\n            filename=filename,\n            origname=origname,\n        )\n        if pkgdir:\n            spec.submodule_search_locations.insert(0, pkgdir)\n        return spec\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Set __file__, if able.\"\"\"\n        module = _new_module(spec.name)\n        try:\n            filename = spec.loader_state.filename\n        except AttributeError:\n            pass\n        else:\n            if filename:\n                module.__file__ = filename\n        return module\n\n    @staticmethod\n    def exec_module(module):\n        spec = module.__spec__\n        name = spec.name\n        code = _call_with_frames_removed(_imp.get_frozen_object, name)\n        exec(code, module.__dict__)\n\n    @classmethod\n    def load_module(cls, fullname):\n        \"\"\"Load a frozen module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # Warning about deprecation implemented in _load_module_shim().\n        module = _load_module_shim(cls, fullname)\n        info = _imp.find_frozen(fullname)\n        assert info is not None\n        _, ispkg, origname = info\n        module.__origname__ = origname\n        vars(module).pop('__file__', None)\n        if ispkg:\n            module.__path__ = []\n        cls._fix_up_module(module)\n        return module\n\n    @classmethod\n    @_requires_frozen\n    def get_code(cls, fullname):\n        \"\"\"Return the code object for the frozen module.\"\"\"\n        return _imp.get_frozen_object(fullname)\n\n    @classmethod\n    @_requires_frozen\n    def get_source(cls, fullname):\n        \"\"\"Return None as frozen modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_frozen\n    def is_package(cls, fullname):\n        \"\"\"Return True if the frozen module is a package.\"\"\"\n        return _imp.is_frozen_package(fullname)\n\n\n# Import itself ###############################################################\n\nclass _ImportLockContext:\n\n    \"\"\"Context manager for the import lock.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Acquire the import lock.\"\"\"\n        _imp.acquire_lock()\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Release the import lock regardless of any raised exceptions.\"\"\"\n        _imp.release_lock()\n\n\ndef _resolve_name(name, package, level):\n    \"\"\"Resolve a relative module name to an absolute one.\"\"\"\n    bits = package.rsplit('.', level - 1)\n    if len(bits) < level:\n        raise ImportError('attempted relative import beyond top-level package')\n    base = bits[0]\n    return f'{base}.{name}' if name else base\n\n\ndef _find_spec(name, path, target=None):\n    \"\"\"Find a module's spec.\"\"\"\n    meta_path = sys.meta_path\n    if meta_path is None:\n        # PyImport_Cleanup() is running or has been called.\n        raise ImportError(\"sys.meta_path is None, Python is likely \"\n                          \"shutting down\")\n\n    if not meta_path:\n        _warnings.warn('sys.meta_path is empty', ImportWarning)\n\n    # We check sys.modules here for the reload case.  While a passed-in\n    # target will usually indicate a reload there is no guarantee, whereas\n    # sys.modules provides one.\n    is_reload = name in sys.modules\n    for finder in meta_path:\n        with _ImportLockContext():\n            try:\n                find_spec = finder.find_spec\n            except AttributeError:\n                continue\n            else:\n                spec = find_spec(name, path, target)\n        if spec is not None:\n            # The parent import may have already imported this module.\n            if not is_reload and name in sys.modules:\n                module = sys.modules[name]\n                try:\n                    __spec__ = module.__spec__\n                except AttributeError:\n                    # We use the found spec since that is the one that\n                    # we would have used if the parent module hadn't\n                    # beaten us to the punch.\n                    return spec\n                else:\n                    if __spec__ is None:\n                        return spec\n                    else:\n                        return __spec__\n            else:\n                return spec\n    else:\n        return None\n\n\ndef _sanity_check(name, package, level):\n    \"\"\"Verify arguments are \"sane\".\"\"\"\n    if not isinstance(name, str):\n        raise TypeError(f'module name must be str, not {type(name)}')\n    if level < 0:\n        raise ValueError('level must be >= 0')\n    if level > 0:\n        if not isinstance(package, str):\n            raise TypeError('__package__ not set to a string')\n        elif not package:\n            raise ImportError('attempted relative import with no known parent '\n                              'package')\n    if not name and level == 0:\n        raise ValueError('Empty module name')\n\n\n_ERR_MSG_PREFIX = 'No module named '\n_ERR_MSG = _ERR_MSG_PREFIX + '{!r}'\n\ndef _find_and_load_unlocked(name, import_):\n    path = None\n    parent = name.rpartition('.')[0]\n    parent_spec = None\n    if parent:\n        if parent not in sys.modules:\n            _call_with_frames_removed(import_, parent)\n        # Crazy side-effects!\n        if name in sys.modules:\n            return sys.modules[name]\n        parent_module = sys.modules[parent]\n        try:\n            path = parent_module.__path__\n        except AttributeError:\n            msg = f'{_ERR_MSG_PREFIX}{name!r}; {parent!r} is not a package'\n            raise ModuleNotFoundError(msg, name=name) from None\n        parent_spec = parent_module.__spec__\n        child = name.rpartition('.')[2]\n    spec = _find_spec(name, path)\n    if spec is None:\n        raise ModuleNotFoundError(f'{_ERR_MSG_PREFIX}{name!r}', name=name)\n    else:\n        if parent_spec:\n            # Temporarily add child we are currently importing to parent's\n            # _uninitialized_submodules for circular import tracking.\n            parent_spec._uninitialized_submodules.append(child)\n        try:\n            module = _load_unlocked(spec)\n        finally:\n            if parent_spec:\n                parent_spec._uninitialized_submodules.pop()\n    if parent:\n        # Set the module as an attribute on its parent.\n        parent_module = sys.modules[parent]\n        try:\n            setattr(parent_module, child, module)\n        except AttributeError:\n            msg = f\"Cannot set an attribute on {parent!r} for child module {child!r}\"\n            _warnings.warn(msg, ImportWarning)\n    return module\n\n\n_NEEDS_LOADING = object()\n\n\ndef _find_and_load(name, import_):\n    \"\"\"Find and load the module.\"\"\"\n\n    # Optimization: we avoid unneeded module locking if the module\n    # already exists in sys.modules and is fully initialized.\n    module = sys.modules.get(name, _NEEDS_LOADING)\n    if (module is _NEEDS_LOADING or\n        getattr(getattr(module, \"__spec__\", None), \"_initializing\", False)):\n        with _ModuleLockManager(name):\n            module = sys.modules.get(name, _NEEDS_LOADING)\n            if module is _NEEDS_LOADING:\n                return _find_and_load_unlocked(name, import_)\n\n        # Optimization: only call _bootstrap._lock_unlock_module() if\n        # module.__spec__._initializing is True.\n        # NOTE: because of this, initializing must be set *before*\n        # putting the new module in sys.modules.\n        _lock_unlock_module(name)\n\n    if module is None:\n        message = f'import of {name} halted; None in sys.modules'\n        raise ModuleNotFoundError(message, name=name)\n\n    return module\n\n\ndef _gcd_import(name, package=None, level=0):\n    \"\"\"Import and return the module based on its name, the package the call is\n    being made from, and the level adjustment.\n\n    This function represents the greatest common denominator of functionality\n    between import_module and __import__. This includes setting __package__ if\n    the loader did not.\n\n    \"\"\"\n    _sanity_check(name, package, level)\n    if level > 0:\n        name = _resolve_name(name, package, level)\n    return _find_and_load(name, _gcd_import)\n\n\ndef _handle_fromlist(module, fromlist, import_, *, recursive=False):\n    \"\"\"Figure out what __import__ should return.\n\n    The import_ parameter is a callable which takes the name of module to\n    import. It is required to decouple the function from assuming importlib's\n    import implementation is desired.\n\n    \"\"\"\n    # The hell that is fromlist ...\n    # If a package was imported, try to import stuff from fromlist.\n    for x in fromlist:\n        if not isinstance(x, str):\n            if recursive:\n                where = module.__name__ + '.__all__'\n            else:\n                where = \"``from list''\"\n            raise TypeError(f\"Item in {where} must be str, \"\n                            f\"not {type(x).__name__}\")\n        elif x == '*':\n            if not recursive and hasattr(module, '__all__'):\n                _handle_fromlist(module, module.__all__, import_,\n                                 recursive=True)\n        elif not hasattr(module, x):\n            from_name = f'{module.__name__}.{x}'\n            try:\n                _call_with_frames_removed(import_, from_name)\n            except ModuleNotFoundError as exc:\n                # Backwards-compatibility dictates we ignore failed\n                # imports triggered by fromlist for modules that don't\n                # exist.\n                if (exc.name == from_name and\n                    sys.modules.get(from_name, _NEEDS_LOADING) is not None):\n                    continue\n                raise\n    return module\n\n\ndef _calc___package__(globals):\n    \"\"\"Calculate what __package__ should be.\n\n    __package__ is not guaranteed to be defined or could be set to None\n    to represent that its proper value is unknown.\n\n    \"\"\"\n    package = globals.get('__package__')\n    spec = globals.get('__spec__')\n    if package is not None:\n        if spec is not None and package != spec.parent:\n            _warnings.warn(\"__package__ != __spec__.parent \"\n                           f\"({package!r} != {spec.parent!r})\",\n                           DeprecationWarning, stacklevel=3)\n        return package\n    elif spec is not None:\n        return spec.parent\n    else:\n        _warnings.warn(\"can't resolve package from __spec__ or __package__, \"\n                       \"falling back on __name__ and __path__\",\n                       ImportWarning, stacklevel=3)\n        package = globals['__name__']\n        if '__path__' not in globals:\n            package = package.rpartition('.')[0]\n    return package\n\n\ndef __import__(name, globals=None, locals=None, fromlist=(), level=0):\n    \"\"\"Import a module.\n\n    The 'globals' argument is used to infer where the import is occurring from\n    to handle relative imports. The 'locals' argument is ignored. The\n    'fromlist' argument specifies what should exist as attributes on the module\n    being imported (e.g. ``from module import <fromlist>``).  The 'level'\n    argument represents the package location to import from in a relative\n    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).\n\n    \"\"\"\n    if level == 0:\n        module = _gcd_import(name)\n    else:\n        globals_ = globals if globals is not None else {}\n        package = _calc___package__(globals_)\n        module = _gcd_import(name, package, level)\n    if not fromlist:\n        # Return up to the first dot in 'name'. This is complicated by the fact\n        # that 'name' may be relative.\n        if level == 0:\n            return _gcd_import(name.partition('.')[0])\n        elif not name:\n            return module\n        else:\n            # Figure out where to slice the module's name up to the first dot\n            # in 'name'.\n            cut_off = len(name) - len(name.partition('.')[0])\n            # Slice end needs to be positive to alleviate need to special-case\n            # when ``'.' not in name``.\n            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]\n    elif hasattr(module, '__path__'):\n        return _handle_fromlist(module, fromlist, _gcd_import)\n    else:\n        return module\n\n\ndef _builtin_from_name(name):\n    spec = BuiltinImporter.find_spec(name)\n    if spec is None:\n        raise ImportError('no built-in module named ' + name)\n    return _load_unlocked(spec)\n\n\ndef _setup(sys_module, _imp_module):\n    \"\"\"Setup importlib by importing needed built-in modules and injecting them\n    into the global namespace.\n\n    As sys is needed for sys.modules access and _imp is needed to load built-in\n    modules, those two modules must be explicitly passed in.\n\n    \"\"\"\n    global _imp, sys, _blocking_on\n    _imp = _imp_module\n    sys = sys_module\n\n    # Set up the spec for existing builtin/frozen modules.\n    module_type = type(sys)\n    for name, module in sys.modules.items():\n        if isinstance(module, module_type):\n            if name in sys.builtin_module_names:\n                loader = BuiltinImporter\n            elif _imp.is_frozen(name):\n                loader = FrozenImporter\n            else:\n                continue\n            spec = _spec_from_module(module, loader)\n            _init_module_attrs(spec, module)\n            if loader is FrozenImporter:\n                loader._fix_up_module(module)\n\n    # Directly load built-in modules needed during bootstrap.\n    self_module = sys.modules[__name__]\n    for builtin_name in ('_thread', '_warnings', '_weakref'):\n        if builtin_name not in sys.modules:\n            builtin_module = _builtin_from_name(builtin_name)\n        else:\n            builtin_module = sys.modules[builtin_name]\n        setattr(self_module, builtin_name, builtin_module)\n\n    # Instantiation requires _weakref to have been set.\n    _blocking_on = _WeakValueDictionary()\n\n\ndef _install(sys_module, _imp_module):\n    \"\"\"Install importers for builtin and frozen modules\"\"\"\n    _setup(sys_module, _imp_module)\n\n    sys.meta_path.append(BuiltinImporter)\n    sys.meta_path.append(FrozenImporter)\n\n\ndef _install_external_importers():\n    \"\"\"Install importers that require external filesystem access\"\"\"\n    global _bootstrap_external\n    import _frozen_importlib_external\n    _bootstrap_external = _frozen_importlib_external\n    _frozen_importlib_external._install(sys.modules[__name__])\n", 1551], "<frozen runpy>": ["\"\"\"runpy.py - locating and running Python code using the module namespace\n\nProvides support for locating and running Python scripts using the Python\nmodule namespace instead of the native filesystem.\n\nThis allows Python code to play nicely with non-filesystem based PEP 302\nimporters when locating support scripts as well as when importing modules.\n\"\"\"\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#    to implement PEP 338 (Executing Modules as Scripts)\n\n\nimport sys\nimport importlib.machinery # importlib first so we can test #15386 via -m\nimport importlib.util\nimport io\nimport os\n\n__all__ = [\n    \"run_module\", \"run_path\",\n]\n\n# avoid 'import types' just for ModuleType\nModuleType = type(sys)\n\nclass _TempModule(object):\n    \"\"\"Temporarily replace a module in sys.modules with an empty namespace\"\"\"\n    def __init__(self, mod_name):\n        self.mod_name = mod_name\n        self.module = ModuleType(mod_name)\n        self._saved_module = []\n\n    def __enter__(self):\n        mod_name = self.mod_name\n        try:\n            self._saved_module.append(sys.modules[mod_name])\n        except KeyError:\n            pass\n        sys.modules[mod_name] = self.module\n        return self\n\n    def __exit__(self, *args):\n        if self._saved_module:\n            sys.modules[self.mod_name] = self._saved_module[0]\n        else:\n            del sys.modules[self.mod_name]\n        self._saved_module = []\n\nclass _ModifiedArgv0(object):\n    def __init__(self, value):\n        self.value = value\n        self._saved_value = self._sentinel = object()\n\n    def __enter__(self):\n        if self._saved_value is not self._sentinel:\n            raise RuntimeError(\"Already preserving saved value\")\n        self._saved_value = sys.argv[0]\n        sys.argv[0] = self.value\n\n    def __exit__(self, *args):\n        self.value = self._sentinel\n        sys.argv[0] = self._saved_value\n\n# TODO: Replace these helpers with importlib._bootstrap_external functions.\ndef _run_code(code, run_globals, init_globals=None,\n              mod_name=None, mod_spec=None,\n              pkg_name=None, script_name=None):\n    \"\"\"Helper to run code in nominated namespace\"\"\"\n    if init_globals is not None:\n        run_globals.update(init_globals)\n    if mod_spec is None:\n        loader = None\n        fname = script_name\n        cached = None\n    else:\n        loader = mod_spec.loader\n        fname = mod_spec.origin\n        cached = mod_spec.cached\n        if pkg_name is None:\n            pkg_name = mod_spec.parent\n    run_globals.update(__name__ = mod_name,\n                       __file__ = fname,\n                       __cached__ = cached,\n                       __doc__ = None,\n                       __loader__ = loader,\n                       __package__ = pkg_name,\n                       __spec__ = mod_spec)\n    exec(code, run_globals)\n    return run_globals\n\ndef _run_module_code(code, init_globals=None,\n                    mod_name=None, mod_spec=None,\n                    pkg_name=None, script_name=None):\n    \"\"\"Helper to run code in new namespace with sys modified\"\"\"\n    fname = script_name if mod_spec is None else mod_spec.origin\n    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):\n        mod_globals = temp_module.module.__dict__\n        _run_code(code, mod_globals, init_globals,\n                  mod_name, mod_spec, pkg_name, script_name)\n    # Copy the globals of the temporary module, as they\n    # may be cleared when the temporary module goes away\n    return mod_globals.copy()\n\n# Helper to get the full name, spec and code for a module\ndef _get_module_details(mod_name, error=ImportError):\n    if mod_name.startswith(\".\"):\n        raise error(\"Relative module names not supported\")\n    pkg_name, _, _ = mod_name.rpartition(\".\")\n    if pkg_name:\n        # Try importing the parent to avoid catching initialization errors\n        try:\n            __import__(pkg_name)\n        except ImportError as e:\n            # If the parent or higher ancestor package is missing, let the\n            # error be raised by find_spec() below and then be caught. But do\n            # not allow other errors to be caught.\n            if e.name is None or (e.name != pkg_name and\n                    not pkg_name.startswith(e.name + \".\")):\n                raise\n        # Warn if the module has already been imported under its normal name\n        existing = sys.modules.get(mod_name)\n        if existing is not None and not hasattr(existing, \"__path__\"):\n            from warnings import warn\n            msg = \"{mod_name!r} found in sys.modules after import of \" \\\n                \"package {pkg_name!r}, but prior to execution of \" \\\n                \"{mod_name!r}; this may result in unpredictable \" \\\n                \"behaviour\".format(mod_name=mod_name, pkg_name=pkg_name)\n            warn(RuntimeWarning(msg))\n\n    try:\n        spec = importlib.util.find_spec(mod_name)\n    except (ImportError, AttributeError, TypeError, ValueError) as ex:\n        # This hack fixes an impedance mismatch between pkgutil and\n        # importlib, where the latter raises other errors for cases where\n        # pkgutil previously raised ImportError\n        msg = \"Error while finding module specification for {!r} ({}: {})\"\n        if mod_name.endswith(\".py\"):\n            msg += (f\". Try using '{mod_name[:-3]}' instead of \"\n                    f\"'{mod_name}' as the module name.\")\n        raise error(msg.format(mod_name, type(ex).__name__, ex)) from ex\n    if spec is None:\n        raise error(\"No module named %s\" % mod_name)\n    if spec.submodule_search_locations is not None:\n        if mod_name == \"__main__\" or mod_name.endswith(\".__main__\"):\n            raise error(\"Cannot use package as __main__ module\")\n        try:\n            pkg_main_name = mod_name + \".__main__\"\n            return _get_module_details(pkg_main_name, error)\n        except error as e:\n            if mod_name not in sys.modules:\n                raise  # No module loaded; being a package is irrelevant\n            raise error((\"%s; %r is a package and cannot \" +\n                               \"be directly executed\") %(e, mod_name))\n    loader = spec.loader\n    if loader is None:\n        raise error(\"%r is a namespace package and cannot be executed\"\n                                                                 % mod_name)\n    try:\n        code = loader.get_code(mod_name)\n    except ImportError as e:\n        raise error(format(e)) from e\n    if code is None:\n        raise error(\"No code object available for %s\" % mod_name)\n    return mod_name, spec, code\n\nclass _Error(Exception):\n    \"\"\"Error that _run_module_as_main() should report without a traceback\"\"\"\n\n# XXX ncoghlan: Should this be documented and made public?\n# (Current thoughts: don't repeat the mistake that lead to its\n# creation when run_module() no longer met the needs of\n# mainmodule.c, but couldn't be changed because it was public)\ndef _run_module_as_main(mod_name, alter_argv=True):\n    \"\"\"Runs the designated module in the __main__ namespace\n\n       Note that the executed module will have full access to the\n       __main__ namespace. If this is not desirable, the run_module()\n       function should be used to run the module code in a fresh namespace.\n\n       At the very least, these variables in __main__ will be overwritten:\n           __name__\n           __file__\n           __cached__\n           __loader__\n           __package__\n    \"\"\"\n    try:\n        if alter_argv or mod_name != \"__main__\": # i.e. -m switch\n            mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n        else:          # i.e. directory or zipfile execution\n            mod_name, mod_spec, code = _get_main_module_details(_Error)\n    except _Error as exc:\n        msg = \"%s: %s\" % (sys.executable, exc)\n        sys.exit(msg)\n    main_globals = sys.modules[\"__main__\"].__dict__\n    if alter_argv:\n        sys.argv[0] = mod_spec.origin\n    return _run_code(code, main_globals, None,\n                     \"__main__\", mod_spec)\n\ndef run_module(mod_name, init_globals=None,\n               run_name=None, alter_sys=False):\n    \"\"\"Execute a module's code without importing it.\n\n       mod_name -- an absolute module name or package name.\n\n       Optional arguments:\n       init_globals -- dictionary used to pre-populate the module\u2019s\n       globals dictionary before the code is executed.\n\n       run_name -- if not None, this will be used for setting __name__;\n       otherwise, __name__ will be set to mod_name + '__main__' if the\n       named module is a package and to just mod_name otherwise.\n\n       alter_sys -- if True, sys.argv[0] is updated with the value of\n       __file__ and sys.modules[__name__] is updated with a temporary\n       module object for the module being executed. Both are\n       restored to their original values before the function returns.\n\n       Returns the resulting module globals dictionary.\n    \"\"\"\n    mod_name, mod_spec, code = _get_module_details(mod_name)\n    if run_name is None:\n        run_name = mod_name\n    if alter_sys:\n        return _run_module_code(code, init_globals, run_name, mod_spec)\n    else:\n        # Leave the sys module alone\n        return _run_code(code, {}, init_globals, run_name, mod_spec)\n\ndef _get_main_module_details(error=ImportError):\n    # Helper that gives a nicer error message when attempting to\n    # execute a zipfile or directory by invoking __main__.py\n    # Also moves the standard __main__ out of the way so that the\n    # preexisting __loader__ entry doesn't cause issues\n    main_name = \"__main__\"\n    saved_main = sys.modules[main_name]\n    del sys.modules[main_name]\n    try:\n        return _get_module_details(main_name)\n    except ImportError as exc:\n        if main_name in str(exc):\n            raise error(\"can't find %r module in %r\" %\n                              (main_name, sys.path[0])) from exc\n        raise\n    finally:\n        sys.modules[main_name] = saved_main\n\n\ndef _get_code_from_file(fname):\n    # Check for a compiled file first\n    from pkgutil import read_code\n    code_path = os.path.abspath(fname)\n    with io.open_code(code_path) as f:\n        code = read_code(f)\n    if code is None:\n        # That didn't work, so try it as normal source code\n        with io.open_code(code_path) as f:\n            code = compile(f.read(), fname, 'exec')\n    return code\n\ndef run_path(path_name, init_globals=None, run_name=None):\n    \"\"\"Execute code located at the specified filesystem location.\n\n       path_name -- filesystem location of a Python script, zipfile,\n       or directory containing a top level __main__.py script.\n\n       Optional arguments:\n       init_globals -- dictionary used to pre-populate the module\u2019s\n       globals dictionary before the code is executed.\n\n       run_name -- if not None, this will be used to set __name__;\n       otherwise, '<run_path>' will be used for __name__.\n\n       Returns the resulting module globals dictionary.\n    \"\"\"\n    if run_name is None:\n        run_name = \"<run_path>\"\n    pkg_name = run_name.rpartition(\".\")[0]\n    from pkgutil import get_importer\n    importer = get_importer(path_name)\n    path_name = os.fsdecode(path_name)\n    if isinstance(importer, type(None)):\n        # Not a valid sys.path entry, so run the code directly\n        # execfile() doesn't help as we want to allow compiled files\n        code = _get_code_from_file(path_name)\n        return _run_module_code(code, init_globals, run_name,\n                                pkg_name=pkg_name, script_name=path_name)\n    else:\n        # Finder is defined for path, so add it to\n        # the start of sys.path\n        sys.path.insert(0, path_name)\n        try:\n            # Here's where things are a little different from the run_module\n            # case. There, we only had to replace the module in sys while the\n            # code was running and doing so was somewhat optional. Here, we\n            # have no choice and we have to remove it even while we read the\n            # code. If we don't do this, a __loader__ attribute in the\n            # existing __main__ module may prevent location of the new module.\n            mod_name, mod_spec, code = _get_main_module_details()\n            with _TempModule(run_name) as temp_module, \\\n                 _ModifiedArgv0(path_name):\n                mod_globals = temp_module.module.__dict__\n                return _run_code(code, mod_globals, init_globals,\n                                    run_name, mod_spec, pkg_name).copy()\n        finally:\n            try:\n                sys.path.remove(path_name)\n            except ValueError:\n                pass\n\n\nif __name__ == \"__main__\":\n    # Run the module specified as the next command line argument\n    if len(sys.argv) < 2:\n        print(\"No module specified for execution\", file=sys.stderr)\n    else:\n        del sys.argv[0] # Make the requested module sys.argv[0]\n        _run_module_as_main(sys.argv[0])\n", 319], "<frozen importlib._bootstrap_external>": ["\"\"\"Core implementation of path-based import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module in the early\n# stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\n# Module injected manually by _set_bootstrap_module()\n_bootstrap = None\n\n# Import builtin modules\nimport _imp\nimport _io\nimport sys\nimport _warnings\nimport marshal\n\n\n_MS_WINDOWS = (sys.platform == 'win32')\nif _MS_WINDOWS:\n    import nt as _os\n    import winreg\nelse:\n    import posix as _os\n\n\nif _MS_WINDOWS:\n    path_separators = ['\\\\', '/']\nelse:\n    path_separators = ['/']\n# Assumption made in _path_join()\nassert all(len(sep) == 1 for sep in path_separators)\npath_sep = path_separators[0]\npath_sep_tuple = tuple(path_separators)\npath_separators = ''.join(path_separators)\n_pathseps_with_colon = {f':{s}' for s in path_separators}\n\n\n# Bootstrap-related code ######################################################\n_CASE_INSENSITIVE_PLATFORMS_STR_KEY = 'win',\n_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY = 'cygwin', 'darwin'\n_CASE_INSENSITIVE_PLATFORMS =  (_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY\n                                + _CASE_INSENSITIVE_PLATFORMS_STR_KEY)\n\n\ndef _make_relax_case():\n    if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS_STR_KEY):\n            key = 'PYTHONCASEOK'\n        else:\n            key = b'PYTHONCASEOK'\n\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively and ignore environment flags are not set.\"\"\"\n            return not sys.flags.ignore_environment and key in _os.environ\n    else:\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively.\"\"\"\n            return False\n    return _relax_case\n\n_relax_case = _make_relax_case()\n\n\ndef _pack_uint32(x):\n    \"\"\"Convert a 32-bit integer to little-endian.\"\"\"\n    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')\n\n\ndef _unpack_uint32(data):\n    \"\"\"Convert 4 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 4\n    return int.from_bytes(data, 'little')\n\ndef _unpack_uint16(data):\n    \"\"\"Convert 2 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 2\n    return int.from_bytes(data, 'little')\n\n\nif _MS_WINDOWS:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        if not path_parts:\n            return \"\"\n        if len(path_parts) == 1:\n            return path_parts[0]\n        root = \"\"\n        path = []\n        for new_root, tail in map(_os._path_splitroot, path_parts):\n            if new_root.startswith(path_sep_tuple) or new_root.endswith(path_sep_tuple):\n                root = new_root.rstrip(path_separators) or root\n                path = [path_sep + tail]\n            elif new_root.endswith(':'):\n                if root.casefold() != new_root.casefold():\n                    # Drive relative paths have to be resolved by the OS, so we reset the\n                    # tail but do not add a path_sep prefix.\n                    root = new_root\n                    path = [tail]\n                else:\n                    path.append(tail)\n            else:\n                root = new_root or root\n                path.append(tail)\n        path = [p.rstrip(path_separators) for p in path if p]\n        if len(path) == 1 and not path[0]:\n            # Avoid losing the root's trailing separator when joining with nothing\n            return root + path_sep\n        return root + path_sep.join(path)\n\nelse:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        return path_sep.join([part.rstrip(path_separators)\n                              for part in path_parts if part])\n\n\ndef _path_split(path):\n    \"\"\"Replacement for os.path.split().\"\"\"\n    i = max(path.rfind(p) for p in path_separators)\n    if i < 0:\n        return '', path\n    return path[:i], path[i + 1:]\n\n\ndef _path_stat(path):\n    \"\"\"Stat the path.\n\n    Made a separate function to make it easier to override in experiments\n    (e.g. cache stat results).\n\n    \"\"\"\n    return _os.stat(path)\n\n\ndef _path_is_mode_type(path, mode):\n    \"\"\"Test whether the path is the specified mode type.\"\"\"\n    try:\n        stat_info = _path_stat(path)\n    except OSError:\n        return False\n    return (stat_info.st_mode & 0o170000) == mode\n\n\ndef _path_isfile(path):\n    \"\"\"Replacement for os.path.isfile.\"\"\"\n    return _path_is_mode_type(path, 0o100000)\n\n\ndef _path_isdir(path):\n    \"\"\"Replacement for os.path.isdir.\"\"\"\n    if not path:\n        path = _os.getcwd()\n    return _path_is_mode_type(path, 0o040000)\n\n\nif _MS_WINDOWS:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        if not path:\n            return False\n        root = _os._path_splitroot(path)[0].replace('/', '\\\\')\n        return len(root) > 1 and (root.startswith('\\\\\\\\') or root.endswith('\\\\'))\n\nelse:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        return path.startswith(path_separators)\n\n\ndef _path_abspath(path):\n    \"\"\"Replacement for os.path.abspath.\"\"\"\n    if not _path_isabs(path):\n        for sep in path_separators:\n            path = path.removeprefix(f\".{sep}\")\n        return _path_join(_os.getcwd(), path)\n    else:\n        return path\n\n\ndef _write_atomic(path, data, mode=0o666):\n    \"\"\"Best-effort function to write data to a path atomically.\n    Be prepared to handle a FileExistsError if concurrent writing of the\n    temporary file is attempted.\"\"\"\n    # id() is used to generate a pseudo-random filename.\n    path_tmp = f'{path}.{id(path)}'\n    fd = _os.open(path_tmp,\n                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)\n    try:\n        # We first write data to a temporary file, and then use os.replace() to\n        # perform an atomic rename.\n        with _io.FileIO(fd, 'wb') as file:\n            bytes_written = file.write(data)\n        if bytes_written != len(data):\n            # Raise an OSError so the 'except' below cleans up the partially\n            # written file.\n            raise OSError(\"os.write() didn't write the full pyc file\")\n        _os.replace(path_tmp, path)\n    except OSError:\n        try:\n            _os.unlink(path_tmp)\n        except OSError:\n            pass\n        raise\n\n\n_code_type = type(_write_atomic.__code__)\n\n\n# Finder/loader utility code ###############################################\n\n# Magic word to reject .pyc files generated by other Python versions.\n# It should change for each incompatible change to the bytecode.\n#\n# The value of CR and LF is incorporated so if you ever read or write\n# a .pyc file in text mode the magic number will be wrong; also, the\n# Apple MPW compiler swaps their values, botching string constants.\n#\n# There were a variety of old schemes for setting the magic number.\n# The current working scheme is to increment the previous value by\n# 10.\n#\n# Starting with the adoption of PEP 3147 in Python 3.2, every bump in magic\n# number also includes a new \"magic tag\", i.e. a human readable string used\n# to represent the magic number in __pycache__ directories.  When you change\n# the magic number, you must also set a new unique magic tag.  Generally this\n# can be named after the Python major version of the magic number bump, but\n# it can really be anything, as long as it's different than anything else\n# that's come before.  The tags are included in the following table, starting\n# with Python 3.2a0.\n#\n# Known values:\n#  Python 1.5:   20121\n#  Python 1.5.1: 20121\n#     Python 1.5.2: 20121\n#     Python 1.6:   50428\n#     Python 2.0:   50823\n#     Python 2.0.1: 50823\n#     Python 2.1:   60202\n#     Python 2.1.1: 60202\n#     Python 2.1.2: 60202\n#     Python 2.2:   60717\n#     Python 2.3a0: 62011\n#     Python 2.3a0: 62021\n#     Python 2.3a0: 62011 (!)\n#     Python 2.4a0: 62041\n#     Python 2.4a3: 62051\n#     Python 2.4b1: 62061\n#     Python 2.5a0: 62071\n#     Python 2.5a0: 62081 (ast-branch)\n#     Python 2.5a0: 62091 (with)\n#     Python 2.5a0: 62092 (changed WITH_CLEANUP opcode)\n#     Python 2.5b3: 62101 (fix wrong code: for x, in ...)\n#     Python 2.5b3: 62111 (fix wrong code: x += yield)\n#     Python 2.5c1: 62121 (fix wrong lnotab with for loops and\n#                          storing constants that should have been removed)\n#     Python 2.5c2: 62131 (fix wrong code: for x, in ... in listcomp/genexp)\n#     Python 2.6a0: 62151 (peephole optimizations and STORE_MAP opcode)\n#     Python 2.6a1: 62161 (WITH_CLEANUP optimization)\n#     Python 2.7a0: 62171 (optimize list comprehensions/change LIST_APPEND)\n#     Python 2.7a0: 62181 (optimize conditional branches:\n#                          introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE)\n#     Python 2.7a0  62191 (introduce SETUP_WITH)\n#     Python 2.7a0  62201 (introduce BUILD_SET)\n#     Python 2.7a0  62211 (introduce MAP_ADD and SET_ADD)\n#     Python 3000:   3000\n#                    3010 (removed UNARY_CONVERT)\n#                    3020 (added BUILD_SET)\n#                    3030 (added keyword-only parameters)\n#                    3040 (added signature annotations)\n#                    3050 (print becomes a function)\n#                    3060 (PEP 3115 metaclass syntax)\n#                    3061 (string literals become unicode)\n#                    3071 (PEP 3109 raise changes)\n#                    3081 (PEP 3137 make __file__ and __name__ unicode)\n#                    3091 (kill str8 interning)\n#                    3101 (merge from 2.6a0, see 62151)\n#                    3103 (__file__ points to source file)\n#     Python 3.0a4: 3111 (WITH_CLEANUP optimization).\n#     Python 3.0b1: 3131 (lexical exception stacking, including POP_EXCEPT\n                          #3021)\n#     Python 3.1a1: 3141 (optimize list, set and dict comprehensions:\n#                         change LIST_APPEND and SET_ADD, add MAP_ADD #2183)\n#     Python 3.1a1: 3151 (optimize conditional branches:\n#                         introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE\n                          #4715)\n#     Python 3.2a1: 3160 (add SETUP_WITH #6101)\n#                   tag: cpython-32\n#     Python 3.2a2: 3170 (add DUP_TOP_TWO, remove DUP_TOPX and ROT_FOUR #9225)\n#                   tag: cpython-32\n#     Python 3.2a3  3180 (add DELETE_DEREF #4617)\n#     Python 3.3a1  3190 (__class__ super closure changed)\n#     Python 3.3a1  3200 (PEP 3155 __qualname__ added #13448)\n#     Python 3.3a1  3210 (added size modulo 2**32 to the pyc header #13645)\n#     Python 3.3a2  3220 (changed PEP 380 implementation #14230)\n#     Python 3.3a4  3230 (revert changes to implicit __class__ closure #14857)\n#     Python 3.4a1  3250 (evaluate positional default arguments before\n#                        keyword-only defaults #16967)\n#     Python 3.4a1  3260 (add LOAD_CLASSDEREF; allow locals of class to override\n#                        free vars #17853)\n#     Python 3.4a1  3270 (various tweaks to the __class__ closure #12370)\n#     Python 3.4a1  3280 (remove implicit class argument)\n#     Python 3.4a4  3290 (changes to __qualname__ computation #19301)\n#     Python 3.4a4  3300 (more changes to __qualname__ computation #19301)\n#     Python 3.4rc2 3310 (alter __qualname__ computation #20625)\n#     Python 3.5a1  3320 (PEP 465: Matrix multiplication operator #21176)\n#     Python 3.5b1  3330 (PEP 448: Additional Unpacking Generalizations #2292)\n#     Python 3.5b2  3340 (fix dictionary display evaluation order #11205)\n#     Python 3.5b3  3350 (add GET_YIELD_FROM_ITER opcode #24400)\n#     Python 3.5.2  3351 (fix BUILD_MAP_UNPACK_WITH_CALL opcode #27286)\n#     Python 3.6a0  3360 (add FORMAT_VALUE opcode #25483)\n#     Python 3.6a1  3361 (lineno delta of code.co_lnotab becomes signed #26107)\n#     Python 3.6a2  3370 (16 bit wordcode #26647)\n#     Python 3.6a2  3371 (add BUILD_CONST_KEY_MAP opcode #27140)\n#     Python 3.6a2  3372 (MAKE_FUNCTION simplification, remove MAKE_CLOSURE\n#                         #27095)\n#     Python 3.6b1  3373 (add BUILD_STRING opcode #27078)\n#     Python 3.6b1  3375 (add SETUP_ANNOTATIONS and STORE_ANNOTATION opcodes\n#                         #27985)\n#     Python 3.6b1  3376 (simplify CALL_FUNCTIONs & BUILD_MAP_UNPACK_WITH_CALL\n                          #27213)\n#     Python 3.6b1  3377 (set __class__ cell from type.__new__ #23722)\n#     Python 3.6b2  3378 (add BUILD_TUPLE_UNPACK_WITH_CALL #28257)\n#     Python 3.6rc1 3379 (more thorough __class__ validation #23722)\n#     Python 3.7a1  3390 (add LOAD_METHOD and CALL_METHOD opcodes #26110)\n#     Python 3.7a2  3391 (update GET_AITER #31709)\n#     Python 3.7a4  3392 (PEP 552: Deterministic pycs #31650)\n#     Python 3.7b1  3393 (remove STORE_ANNOTATION opcode #32550)\n#     Python 3.7b5  3394 (restored docstring as the first stmt in the body;\n#                         this might affected the first line number #32911)\n#     Python 3.8a1  3400 (move frame block handling to compiler #17611)\n#     Python 3.8a1  3401 (add END_ASYNC_FOR #33041)\n#     Python 3.8a1  3410 (PEP570 Python Positional-Only Parameters #36540)\n#     Python 3.8b2  3411 (Reverse evaluation order of key: value in dict\n#                         comprehensions #35224)\n#     Python 3.8b2  3412 (Swap the position of positional args and positional\n#                         only args in ast.arguments #37593)\n#     Python 3.8b4  3413 (Fix \"break\" and \"continue\" in \"finally\" #37830)\n#     Python 3.9a0  3420 (add LOAD_ASSERTION_ERROR #34880)\n#     Python 3.9a0  3421 (simplified bytecode for with blocks #32949)\n#     Python 3.9a0  3422 (remove BEGIN_FINALLY, END_FINALLY, CALL_FINALLY, POP_FINALLY bytecodes #33387)\n#     Python 3.9a2  3423 (add IS_OP, CONTAINS_OP and JUMP_IF_NOT_EXC_MATCH bytecodes #39156)\n#     Python 3.9a2  3424 (simplify bytecodes for *value unpacking)\n#     Python 3.9a2  3425 (simplify bytecodes for **value unpacking)\n#     Python 3.10a1 3430 (Make 'annotations' future by default)\n#     Python 3.10a1 3431 (New line number table format -- PEP 626)\n#     Python 3.10a2 3432 (Function annotation for MAKE_FUNCTION is changed from dict to tuple bpo-42202)\n#     Python 3.10a2 3433 (RERAISE restores f_lasti if oparg != 0)\n#     Python 3.10a6 3434 (PEP 634: Structural Pattern Matching)\n#     Python 3.10a7 3435 Use instruction offsets (as opposed to byte offsets).\n#     Python 3.10b1 3436 (Add GEN_START bytecode #43683)\n#     Python 3.10b1 3437 (Undo making 'annotations' future by default - We like to dance among core devs!)\n#     Python 3.10b1 3438 Safer line number table handling.\n#     Python 3.10b1 3439 (Add ROT_N)\n#     Python 3.11a1 3450 Use exception table for unwinding (\"zero cost\" exception handling)\n#     Python 3.11a1 3451 (Add CALL_METHOD_KW)\n#     Python 3.11a1 3452 (drop nlocals from marshaled code objects)\n#     Python 3.11a1 3453 (add co_fastlocalnames and co_fastlocalkinds)\n#     Python 3.11a1 3454 (compute cell offsets relative to locals bpo-43693)\n#     Python 3.11a1 3455 (add MAKE_CELL bpo-43693)\n#     Python 3.11a1 3456 (interleave cell args bpo-43693)\n#     Python 3.11a1 3457 (Change localsplus to a bytes object bpo-43693)\n#     Python 3.11a1 3458 (imported objects now don't use LOAD_METHOD/CALL_METHOD)\n#     Python 3.11a1 3459 (PEP 657: add end line numbers and column offsets for instructions)\n#     Python 3.11a1 3460 (Add co_qualname field to PyCodeObject bpo-44530)\n#     Python 3.11a1 3461 (JUMP_ABSOLUTE must jump backwards)\n#     Python 3.11a2 3462 (bpo-44511: remove COPY_DICT_WITHOUT_KEYS, change\n#                         MATCH_CLASS and MATCH_KEYS, and add COPY)\n#     Python 3.11a3 3463 (bpo-45711: JUMP_IF_NOT_EXC_MATCH no longer pops the\n#                         active exception)\n#     Python 3.11a3 3464 (bpo-45636: Merge numeric BINARY_*/INPLACE_* into\n#                         BINARY_OP)\n#     Python 3.11a3 3465 (Add COPY_FREE_VARS opcode)\n#     Python 3.11a4 3466 (bpo-45292: PEP-654 except*)\n#     Python 3.11a4 3467 (Change CALL_xxx opcodes)\n#     Python 3.11a4 3468 (Add SEND opcode)\n#     Python 3.11a4 3469 (bpo-45711: remove type, traceback from exc_info)\n#     Python 3.11a4 3470 (bpo-46221: PREP_RERAISE_STAR no longer pushes lasti)\n#     Python 3.11a4 3471 (bpo-46202: remove pop POP_EXCEPT_AND_RERAISE)\n#     Python 3.11a4 3472 (bpo-46009: replace GEN_START with POP_TOP)\n#     Python 3.11a4 3473 (Add POP_JUMP_IF_NOT_NONE/POP_JUMP_IF_NONE opcodes)\n#     Python 3.11a4 3474 (Add RESUME opcode)\n#     Python 3.11a5 3475 (Add RETURN_GENERATOR opcode)\n#     Python 3.11a5 3476 (Add ASYNC_GEN_WRAP opcode)\n#     Python 3.11a5 3477 (Replace DUP_TOP/DUP_TOP_TWO with COPY and\n#                         ROT_TWO/ROT_THREE/ROT_FOUR/ROT_N with SWAP)\n#     Python 3.11a5 3478 (New CALL opcodes)\n#     Python 3.11a5 3479 (Add PUSH_NULL opcode)\n#     Python 3.11a5 3480 (New CALL opcodes, second iteration)\n#     Python 3.11a5 3481 (Use inline cache for BINARY_OP)\n#     Python 3.11a5 3482 (Use inline caching for UNPACK_SEQUENCE and LOAD_GLOBAL)\n#     Python 3.11a5 3483 (Use inline caching for COMPARE_OP and BINARY_SUBSCR)\n#     Python 3.11a5 3484 (Use inline caching for LOAD_ATTR, LOAD_METHOD, and\n#                         STORE_ATTR)\n#     Python 3.11a5 3485 (Add an oparg to GET_AWAITABLE)\n#     Python 3.11a6 3486 (Use inline caching for PRECALL and CALL)\n#     Python 3.11a6 3487 (Remove the adaptive \"oparg counter\" mechanism)\n#     Python 3.11a6 3488 (LOAD_GLOBAL can push additional NULL)\n#     Python 3.11a6 3489 (Add JUMP_BACKWARD, remove JUMP_ABSOLUTE)\n#     Python 3.11a6 3490 (remove JUMP_IF_NOT_EXC_MATCH, add CHECK_EXC_MATCH)\n#     Python 3.11a6 3491 (remove JUMP_IF_NOT_EG_MATCH, add CHECK_EG_MATCH,\n#                         add JUMP_BACKWARD_NO_INTERRUPT, make JUMP_NO_INTERRUPT virtual)\n#     Python 3.11a7 3492 (make POP_JUMP_IF_NONE/NOT_NONE/TRUE/FALSE relative)\n#     Python 3.11a7 3493 (Make JUMP_IF_TRUE_OR_POP/JUMP_IF_FALSE_OR_POP relative)\n#     Python 3.11a7 3494 (New location info table)\n#     Python 3.11b4 3495 (Set line number of module's RESUME instr to 0 per PEP 626)\n#     Python 3.12a1 3500 (Remove PRECALL opcode)\n#     Python 3.12a1 3501 (YIELD_VALUE oparg == stack_depth)\n#     Python 3.12a1 3502 (LOAD_FAST_CHECK, no NULL-check in LOAD_FAST)\n#     Python 3.12a1 3503 (Shrink LOAD_METHOD cache)\n#     Python 3.12a1 3504 (Merge LOAD_METHOD back into LOAD_ATTR)\n#     Python 3.12a1 3505 (Specialization/Cache for FOR_ITER)\n#     Python 3.12a1 3506 (Add BINARY_SLICE and STORE_SLICE instructions)\n#     Python 3.12a1 3507 (Set lineno of module's RESUME to 0)\n#     Python 3.12a1 3508 (Add CLEANUP_THROW)\n#     Python 3.12a1 3509 (Conditional jumps only jump forward)\n#     Python 3.12a2 3510 (FOR_ITER leaves iterator on the stack)\n#     Python 3.12a2 3511 (Add STOPITERATION_ERROR instruction)\n#     Python 3.12a2 3512 (Remove all unused consts from code objects)\n#     Python 3.12a4 3513 (Add CALL_INTRINSIC_1 instruction, removed STOPITERATION_ERROR, PRINT_EXPR, IMPORT_STAR)\n#     Python 3.12a4 3514 (Remove ASYNC_GEN_WRAP, LIST_TO_TUPLE, and UNARY_POSITIVE)\n#     Python 3.12a5 3515 (Embed jump mask in COMPARE_OP oparg)\n#     Python 3.12a5 3516 (Add COMPARE_AND_BRANCH instruction)\n#     Python 3.12a5 3517 (Change YIELD_VALUE oparg to exception block depth)\n#     Python 3.12a6 3518 (Add RETURN_CONST instruction)\n#     Python 3.12a6 3519 (Modify SEND instruction)\n#     Python 3.12a6 3520 (Remove PREP_RERAISE_STAR, add CALL_INTRINSIC_2)\n#     Python 3.12a7 3521 (Shrink the LOAD_GLOBAL caches)\n#     Python 3.12a7 3522 (Removed JUMP_IF_FALSE_OR_POP/JUMP_IF_TRUE_OR_POP)\n#     Python 3.12a7 3523 (Convert COMPARE_AND_BRANCH back to COMPARE_OP)\n#     Python 3.12a7 3524 (Shrink the BINARY_SUBSCR caches)\n#     Python 3.12b1 3525 (Shrink the CALL caches)\n#     Python 3.12b1 3526 (Add instrumentation support)\n#     Python 3.12b1 3527 (Add LOAD_SUPER_ATTR)\n#     Python 3.12b1 3528 (Add LOAD_SUPER_ATTR_METHOD specialization)\n#     Python 3.12b1 3529 (Inline list/dict/set comprehensions)\n#     Python 3.12b1 3530 (Shrink the LOAD_SUPER_ATTR caches)\n#     Python 3.12b1 3531 (Add PEP 695 changes)\n\n#     Python 3.13 will start with 3550\n\n#     Please don't copy-paste the same pre-release tag for new entries above!!!\n#     You should always use the *upcoming* tag. For example, if 3.12a6 came out\n#     a week ago, I should put \"Python 3.12a7\" next to my new magic number.\n\n# MAGIC must change whenever the bytecode emitted by the compiler may no\n# longer be understood by older implementations of the eval loop (usually\n# due to the addition of new opcodes).\n#\n# Starting with Python 3.11, Python 3.n starts with magic number 2900+50n.\n#\n# Whenever MAGIC_NUMBER is changed, the ranges in the magic_values array\n# in PC/launcher.c must also be updated.\n\nMAGIC_NUMBER = (3531).to_bytes(2, 'little') + b'\\r\\n'\n\n_RAW_MAGIC_NUMBER = int.from_bytes(MAGIC_NUMBER, 'little')  # For import.c\n\n_PYCACHE = '__pycache__'\n_OPT = 'opt-'\n\nSOURCE_SUFFIXES = ['.py']\nif _MS_WINDOWS:\n    SOURCE_SUFFIXES.append('.pyw')\n\nEXTENSION_SUFFIXES = _imp.extension_suffixes()\n\nBYTECODE_SUFFIXES = ['.pyc']\n# Deprecated.\nDEBUG_BYTECODE_SUFFIXES = OPTIMIZED_BYTECODE_SUFFIXES = BYTECODE_SUFFIXES\n\ndef cache_from_source(path, debug_override=None, *, optimization=None):\n    \"\"\"Given the path to a .py file, return the path to its .pyc file.\n\n    The .py file does not need to exist; this simply returns the path to the\n    .pyc file calculated as if the .py file were imported.\n\n    The 'optimization' parameter controls the presumed optimization level of\n    the bytecode file. If 'optimization' is not None, the string representation\n    of the argument is taken and verified to be alphanumeric (else ValueError\n    is raised).\n\n    The debug_override parameter is deprecated. If debug_override is not None,\n    a True value is the same as setting 'optimization' to the empty string\n    while a False value is equivalent to setting 'optimization' to '1'.\n\n    If sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if debug_override is not None:\n        _warnings.warn('the debug_override parameter is deprecated; use '\n                       \"'optimization' instead\", DeprecationWarning)\n        if optimization is not None:\n            message = 'debug_override or optimization must be set to None'\n            raise TypeError(message)\n        optimization = '' if debug_override else 1\n    path = _os.fspath(path)\n    head, tail = _path_split(path)\n    base, sep, rest = tail.rpartition('.')\n    tag = sys.implementation.cache_tag\n    if tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    almost_filename = ''.join([(base if base else rest), sep, tag])\n    if optimization is None:\n        if sys.flags.optimize == 0:\n            optimization = ''\n        else:\n            optimization = sys.flags.optimize\n    optimization = str(optimization)\n    if optimization != '':\n        if not optimization.isalnum():\n            raise ValueError(f'{optimization!r} is not alphanumeric')\n        almost_filename = f'{almost_filename}.{_OPT}{optimization}'\n    filename = almost_filename + BYTECODE_SUFFIXES[0]\n    if sys.pycache_prefix is not None:\n        # We need an absolute path to the py file to avoid the possibility of\n        # collisions within sys.pycache_prefix, if someone has two different\n        # `foo/bar.py` on their system and they import both of them using the\n        # same sys.pycache_prefix. Let's say sys.pycache_prefix is\n        # `C:\\Bytecode`; the idea here is that if we get `Foo\\Bar`, we first\n        # make it absolute (`C:\\Somewhere\\Foo\\Bar`), then make it root-relative\n        # (`Somewhere\\Foo\\Bar`), so we end up placing the bytecode file in an\n        # unambiguous `C:\\Bytecode\\Somewhere\\Foo\\Bar\\`.\n        head = _path_abspath(head)\n\n        # Strip initial drive from a Windows path. We know we have an absolute\n        # path here, so the second part of the check rules out a POSIX path that\n        # happens to contain a colon at the second character.\n        if head[1] == ':' and head[0] not in path_separators:\n            head = head[2:]\n\n        # Strip initial path separator from `head` to complete the conversion\n        # back to a root-relative path before joining.\n        return _path_join(\n            sys.pycache_prefix,\n            head.lstrip(path_separators),\n            filename,\n        )\n    return _path_join(head, _PYCACHE, filename)\n\n\ndef source_from_cache(path):\n    \"\"\"Given the path to a .pyc. file, return the path to its .py file.\n\n    The .pyc file does not need to exist; this simply returns the path to\n    the .py file calculated to correspond to the .pyc file.  If path does\n    not conform to PEP 3147/488 format, ValueError will be raised. If\n    sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if sys.implementation.cache_tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    path = _os.fspath(path)\n    head, pycache_filename = _path_split(path)\n    found_in_pycache_prefix = False\n    if sys.pycache_prefix is not None:\n        stripped_path = sys.pycache_prefix.rstrip(path_separators)\n        if head.startswith(stripped_path + path_sep):\n            head = head[len(stripped_path):]\n            found_in_pycache_prefix = True\n    if not found_in_pycache_prefix:\n        head, pycache = _path_split(head)\n        if pycache != _PYCACHE:\n            raise ValueError(f'{_PYCACHE} not bottom-level directory in '\n                             f'{path!r}')\n    dot_count = pycache_filename.count('.')\n    if dot_count not in {2, 3}:\n        raise ValueError(f'expected only 2 or 3 dots in {pycache_filename!r}')\n    elif dot_count == 3:\n        optimization = pycache_filename.rsplit('.', 2)[-2]\n        if not optimization.startswith(_OPT):\n            raise ValueError(\"optimization portion of filename does not start \"\n                             f\"with {_OPT!r}\")\n        opt_level = optimization[len(_OPT):]\n        if not opt_level.isalnum():\n            raise ValueError(f\"optimization level {optimization!r} is not an \"\n                             \"alphanumeric value\")\n    base_filename = pycache_filename.partition('.')[0]\n    return _path_join(head, base_filename + SOURCE_SUFFIXES[0])\n\n\ndef _get_sourcefile(bytecode_path):\n    \"\"\"Convert a bytecode file path to a source path (if possible).\n\n    This function exists purely for backwards-compatibility for\n    PyImport_ExecCodeModuleWithFilenames() in the C API.\n\n    \"\"\"\n    if len(bytecode_path) == 0:\n        return None\n    rest, _, extension = bytecode_path.rpartition('.')\n    if not rest or extension.lower()[-3:-1] != 'py':\n        return bytecode_path\n    try:\n        source_path = source_from_cache(bytecode_path)\n    except (NotImplementedError, ValueError):\n        source_path = bytecode_path[:-1]\n    return source_path if _path_isfile(source_path) else bytecode_path\n\n\ndef _get_cached(filename):\n    if filename.endswith(tuple(SOURCE_SUFFIXES)):\n        try:\n            return cache_from_source(filename)\n        except NotImplementedError:\n            pass\n    elif filename.endswith(tuple(BYTECODE_SUFFIXES)):\n        return filename\n    else:\n        return None\n\n\ndef _calc_mode(path):\n    \"\"\"Calculate the mode permissions for a bytecode file.\"\"\"\n    try:\n        mode = _path_stat(path).st_mode\n    except OSError:\n        mode = 0o666\n    # We always ensure write access so we can update cached files\n    # later even when the source files are read-only on Windows (#6074)\n    mode |= 0o200\n    return mode\n\n\ndef _check_name(method):\n    \"\"\"Decorator to verify that the module being requested matches the one the\n    loader can handle.\n\n    The first argument (self) must define _name which the second argument is\n    compared against. If the comparison fails then ImportError is raised.\n\n    \"\"\"\n    def _check_name_wrapper(self, name=None, *args, **kwargs):\n        if name is None:\n            name = self.name\n        elif self.name != name:\n            raise ImportError('loader for %s cannot handle %s' %\n                                (self.name, name), name=name)\n        return method(self, name, *args, **kwargs)\n\n    # FIXME: @_check_name is used to define class methods before the\n    # _bootstrap module is set by _set_bootstrap_module().\n    if _bootstrap is not None:\n        _wrap = _bootstrap._wrap\n    else:\n        def _wrap(new, old):\n            for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n                if hasattr(old, replace):\n                    setattr(new, replace, getattr(old, replace))\n            new.__dict__.update(old.__dict__)\n\n    _wrap(_check_name_wrapper, method)\n    return _check_name_wrapper\n\n\ndef _classify_pyc(data, name, exc_details):\n    \"\"\"Perform basic validity checking of a pyc header and return the flags field,\n    which determines how the pyc should be further validated against the source.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required, though.)\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    ImportError is raised when the magic number is incorrect or when the flags\n    field is invalid. EOFError is raised when the data is found to be truncated.\n\n    \"\"\"\n    magic = data[:4]\n    if magic != MAGIC_NUMBER:\n        message = f'bad magic number in {name!r}: {magic!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if len(data) < 16:\n        message = f'reached EOF while reading pyc header of {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise EOFError(message)\n    flags = _unpack_uint32(data[4:8])\n    # Only the first two flags are defined.\n    if flags & ~0b11:\n        message = f'invalid flags {flags!r} in {name!r}'\n        raise ImportError(message, **exc_details)\n    return flags\n\n\ndef _validate_timestamp_pyc(data, source_mtime, source_size, name,\n                            exc_details):\n    \"\"\"Validate a pyc against the source last-modified time.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_mtime* is the last modified timestamp of the source file.\n\n    *source_size* is None or the size of the source file in bytes.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if _unpack_uint32(data[8:12]) != (source_mtime & 0xFFFFFFFF):\n        message = f'bytecode is stale for {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if (source_size is not None and\n        _unpack_uint32(data[12:16]) != (source_size & 0xFFFFFFFF)):\n        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)\n\n\ndef _validate_hash_pyc(data, source_hash, name, exc_details):\n    \"\"\"Validate a hash-based pyc by checking the real source hash against the one in\n    the pyc header.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_hash* is the importlib.util.source_hash() of the source file.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if data[8:16] != source_hash:\n        raise ImportError(\n            f'hash in bytecode doesn\\'t match hash of source {name!r}',\n            **exc_details,\n        )\n\n\ndef _compile_bytecode(data, name=None, bytecode_path=None, source_path=None):\n    \"\"\"Compile bytecode as found in a pyc.\"\"\"\n    code = marshal.loads(data)\n    if isinstance(code, _code_type):\n        _bootstrap._verbose_message('code object from {!r}', bytecode_path)\n        if source_path is not None:\n            _imp._fix_co_filename(code, source_path)\n        return code\n    else:\n        raise ImportError(f'Non-code object in {bytecode_path!r}',\n                          name=name, path=bytecode_path)\n\n\ndef _code_to_timestamp_pyc(code, mtime=0, source_size=0):\n    \"Produce the data for a timestamp-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    data.extend(_pack_uint32(0))\n    data.extend(_pack_uint32(mtime))\n    data.extend(_pack_uint32(source_size))\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef _code_to_hash_pyc(code, source_hash, checked=True):\n    \"Produce the data for a hash-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    flags = 0b1 | checked << 1\n    data.extend(_pack_uint32(flags))\n    assert len(source_hash) == 8\n    data.extend(source_hash)\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef decode_source(source_bytes):\n    \"\"\"Decode bytes representing source code and return the string.\n\n    Universal newline support is used in the decoding.\n    \"\"\"\n    import tokenize  # To avoid bootstrap issues.\n    source_bytes_readline = _io.BytesIO(source_bytes).readline\n    encoding = tokenize.detect_encoding(source_bytes_readline)\n    newline_decoder = _io.IncrementalNewlineDecoder(None, True)\n    return newline_decoder.decode(source_bytes.decode(encoding[0]))\n\n\n# Module specifications #######################################################\n\n_POPULATE = object()\n\n\ndef spec_from_file_location(name, location=None, *, loader=None,\n                            submodule_search_locations=_POPULATE):\n    \"\"\"Return a module spec based on a file location.\n\n    To indicate that the module is a package, set\n    submodule_search_locations to a list of directory paths.  An\n    empty list is sufficient, though its not otherwise useful to the\n    import system.\n\n    The loader must take a spec as its only __init__() arg.\n\n    \"\"\"\n    if location is None:\n        # The caller may simply want a partially populated location-\n        # oriented spec.  So we set the location to a bogus value and\n        # fill in as much as we can.\n        location = '<unknown>'\n        if hasattr(loader, 'get_filename'):\n            # ExecutionLoader\n            try:\n                location = loader.get_filename(name)\n            except ImportError:\n                pass\n    else:\n        location = _os.fspath(location)\n        try:\n            location = _path_abspath(location)\n        except OSError:\n            pass\n\n    # If the location is on the filesystem, but doesn't actually exist,\n    # we could return None here, indicating that the location is not\n    # valid.  However, we don't have a good way of testing since an\n    # indirect location (e.g. a zip file or URL) will look like a\n    # non-existent file relative to the filesystem.\n\n    spec = _bootstrap.ModuleSpec(name, loader, origin=location)\n    spec._set_fileattr = True\n\n    # Pick a loader if one wasn't provided.\n    if loader is None:\n        for loader_class, suffixes in _get_supported_file_loaders():\n            if location.endswith(tuple(suffixes)):\n                loader = loader_class(name, location)\n                spec.loader = loader\n                break\n        else:\n            return None\n\n    # Set submodule_search_paths appropriately.\n    if submodule_search_locations is _POPULATE:\n        # Check the loader.\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                pass\n            else:\n                if is_package:\n                    spec.submodule_search_locations = []\n    else:\n        spec.submodule_search_locations = submodule_search_locations\n    if spec.submodule_search_locations == []:\n        if location:\n            dirname = _path_split(location)[0]\n            spec.submodule_search_locations.append(dirname)\n\n    return spec\n\n\ndef _bless_my_loader(module_globals):\n    \"\"\"Helper function for _warnings.c\n\n    See GH#97850 for details.\n    \"\"\"\n    # 2022-10-06(warsaw): For now, this helper is only used in _warnings.c and\n    # that use case only has the module globals.  This function could be\n    # extended to accept either that or a module object.  However, in the\n    # latter case, it would be better to raise certain exceptions when looking\n    # at a module, which should have either a __loader__ or __spec__.loader.\n    # For backward compatibility, it is possible that we'll get an empty\n    # dictionary for the module globals, and that cannot raise an exception.\n    if not isinstance(module_globals, dict):\n        return None\n\n    missing = object()\n    loader = module_globals.get('__loader__', None)\n    spec = module_globals.get('__spec__', missing)\n\n    if loader is None:\n        if spec is missing:\n            # If working with a module:\n            # raise AttributeError('Module globals is missing a __spec__')\n            return None\n        elif spec is None:\n            raise ValueError('Module globals is missing a __spec__.loader')\n\n    spec_loader = getattr(spec, 'loader', missing)\n\n    if spec_loader in (missing, None):\n        if loader is None:\n            exc = AttributeError if spec_loader is missing else ValueError\n            raise exc('Module globals is missing a __spec__.loader')\n        _warnings.warn(\n            'Module globals is missing a __spec__.loader',\n            DeprecationWarning)\n        spec_loader = loader\n\n    assert spec_loader is not None\n    if loader is not None and loader != spec_loader:\n        _warnings.warn(\n            'Module globals; __loader__ != __spec__.loader',\n            DeprecationWarning)\n        return loader\n\n    return spec_loader\n\n\n# Loaders #####################################################################\n\nclass WindowsRegistryFinder:\n\n    \"\"\"Meta path finder for modules declared in the Windows registry.\"\"\"\n\n    REGISTRY_KEY = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}')\n    REGISTRY_KEY_DEBUG = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}\\\\Debug')\n    DEBUG_BUILD = (_MS_WINDOWS and '_d.pyd' in EXTENSION_SUFFIXES)\n\n    @staticmethod\n    def _open_registry(key):\n        try:\n            return winreg.OpenKey(winreg.HKEY_CURRENT_USER, key)\n        except OSError:\n            return winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key)\n\n    @classmethod\n    def _search_registry(cls, fullname):\n        if cls.DEBUG_BUILD:\n            registry_key = cls.REGISTRY_KEY_DEBUG\n        else:\n            registry_key = cls.REGISTRY_KEY\n        key = registry_key.format(fullname=fullname,\n                                  sys_version='%d.%d' % sys.version_info[:2])\n        try:\n            with cls._open_registry(key) as hkey:\n                filepath = winreg.QueryValue(hkey, '')\n        except OSError:\n            return None\n        return filepath\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        filepath = cls._search_registry(fullname)\n        if filepath is None:\n            return None\n        try:\n            _path_stat(filepath)\n        except OSError:\n            return None\n        for loader, suffixes in _get_supported_file_loaders():\n            if filepath.endswith(tuple(suffixes)):\n                spec = _bootstrap.spec_from_loader(fullname,\n                                                   loader(fullname, filepath),\n                                                   origin=filepath)\n                return spec\n\n\nclass _LoaderBasics:\n\n    \"\"\"Base class of common code needed by both SourceLoader and\n    SourcelessFileLoader.\"\"\"\n\n    def is_package(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.is_package by checking if\n        the path returned by get_filename has a filename of '__init__.py'.\"\"\"\n        filename = _path_split(self.get_filename(fullname))[1]\n        filename_base = filename.rsplit('.', 1)[0]\n        tail_name = fullname.rpartition('.')[2]\n        return filename_base == '__init__' and tail_name != '__init__'\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        \"\"\"Execute the module.\"\"\"\n        code = self.get_code(module.__name__)\n        if code is None:\n            raise ImportError(f'cannot load module {module.__name__!r} when '\n                              'get_code() returns None')\n        _bootstrap._call_with_frames_removed(exec, code, module.__dict__)\n\n    def load_module(self, fullname):\n        \"\"\"This method is deprecated.\"\"\"\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n\nclass SourceLoader(_LoaderBasics):\n\n    def path_mtime(self, path):\n        \"\"\"Optional method that returns the modification time (an int) for the\n        specified path (a str).\n\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        raise OSError\n\n    def path_stats(self, path):\n        \"\"\"Optional method returning a metadata dict for the specified\n        path (a str).\n\n        Possible keys:\n        - 'mtime' (mandatory) is the numeric timestamp of last source\n          code modification;\n        - 'size' (optional) is the size in bytes of the source code.\n\n        Implementing this method allows the loader to read bytecode files.\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        return {'mtime': self.path_mtime(path)}\n\n    def _cache_bytecode(self, source_path, cache_path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n\n        The source path is needed in order to correctly transfer permissions\n        \"\"\"\n        # For backwards compatibility, we delegate to set_data()\n        return self.set_data(cache_path, data)\n\n    def set_data(self, path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n        \"\"\"\n\n\n    def get_source(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_source.\"\"\"\n        path = self.get_filename(fullname)\n        try:\n            source_bytes = self.get_data(path)\n        except OSError as exc:\n            raise ImportError('source not available through get_data()',\n                              name=fullname) from exc\n        return decode_source(source_bytes)\n\n    def source_to_code(self, data, path, *, _optimize=-1):\n        \"\"\"Return the code object compiled from source.\n\n        The 'data' argument can be any object type that compile() supports.\n        \"\"\"\n        return _bootstrap._call_with_frames_removed(compile, data, path, 'exec',\n                                        dont_inherit=True, optimize=_optimize)\n\n    def get_code(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_code.\n\n        Reading of bytecode requires path_stats to be implemented. To write\n        bytecode, set_data must also be implemented.\n\n        \"\"\"\n        source_path = self.get_filename(fullname)\n        source_mtime = None\n        source_bytes = None\n        source_hash = None\n        hash_based = False\n        check_source = True\n        try:\n            bytecode_path = cache_from_source(source_path)\n        except NotImplementedError:\n            bytecode_path = None\n        else:\n            try:\n                st = self.path_stats(source_path)\n            except OSError:\n                pass\n            else:\n                source_mtime = int(st['mtime'])\n                try:\n                    data = self.get_data(bytecode_path)\n                except OSError:\n                    pass\n                else:\n                    exc_details = {\n                        'name': fullname,\n                        'path': bytecode_path,\n                    }\n                    try:\n                        flags = _classify_pyc(data, fullname, exc_details)\n                        bytes_data = memoryview(data)[16:]\n                        hash_based = flags & 0b1 != 0\n                        if hash_based:\n                            check_source = flags & 0b10 != 0\n                            if (_imp.check_hash_based_pycs != 'never' and\n                                (check_source or\n                                 _imp.check_hash_based_pycs == 'always')):\n                                source_bytes = self.get_data(source_path)\n                                source_hash = _imp.source_hash(\n                                    _RAW_MAGIC_NUMBER,\n                                    source_bytes,\n                                )\n                                _validate_hash_pyc(data, source_hash, fullname,\n                                                   exc_details)\n                        else:\n                            _validate_timestamp_pyc(\n                                data,\n                                source_mtime,\n                                st['size'],\n                                fullname,\n                                exc_details,\n                            )\n                    except (ImportError, EOFError):\n                        pass\n                    else:\n                        _bootstrap._verbose_message('{} matches {}', bytecode_path,\n                                                    source_path)\n                        return _compile_bytecode(bytes_data, name=fullname,\n                                                 bytecode_path=bytecode_path,\n                                                 source_path=source_path)\n        if source_bytes is None:\n            source_bytes = self.get_data(source_path)\n        code_object = self.source_to_code(source_bytes, source_path)\n        _bootstrap._verbose_message('code object from {}', source_path)\n        if (not sys.dont_write_bytecode and bytecode_path is not None and\n                source_mtime is not None):\n            if hash_based:\n                if source_hash is None:\n                    source_hash = _imp.source_hash(_RAW_MAGIC_NUMBER,\n                                                   source_bytes)\n                data = _code_to_hash_pyc(code_object, source_hash, check_source)\n            else:\n                data = _code_to_timestamp_pyc(code_object, source_mtime,\n                                              len(source_bytes))\n            try:\n                self._cache_bytecode(source_path, bytecode_path, data)\n            except NotImplementedError:\n                pass\n        return code_object\n\n\nclass FileLoader:\n\n    \"\"\"Base file loader class which implements the loader protocol methods that\n    require file system usage.\"\"\"\n\n    def __init__(self, fullname, path):\n        \"\"\"Cache the module name and the path to the file found by the\n        finder.\"\"\"\n        self.name = fullname\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    @_check_name\n    def load_module(self, fullname):\n        \"\"\"Load a module from a file.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The only reason for this method is for the name check.\n        # Issue #14857: Avoid the zero-argument form of super so the implementation\n        # of that form can be updated without breaking the frozen module.\n        return super(FileLoader, self).load_module(fullname)\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n    def get_data(self, path):\n        \"\"\"Return the data from path as raw bytes.\"\"\"\n        if isinstance(self, (SourceLoader, ExtensionFileLoader)):\n            with _io.open_code(str(path)) as file:\n                return file.read()\n        else:\n            with _io.FileIO(path, 'r') as file:\n                return file.read()\n\n    @_check_name\n    def get_resource_reader(self, module):\n        from importlib.readers import FileReader\n        return FileReader(self)\n\n\nclass SourceFileLoader(FileLoader, SourceLoader):\n\n    \"\"\"Concrete implementation of SourceLoader using the file system.\"\"\"\n\n    def path_stats(self, path):\n        \"\"\"Return the metadata for the path.\"\"\"\n        st = _path_stat(path)\n        return {'mtime': st.st_mtime, 'size': st.st_size}\n\n    def _cache_bytecode(self, source_path, bytecode_path, data):\n        # Adapt between the two APIs\n        mode = _calc_mode(source_path)\n        return self.set_data(bytecode_path, data, _mode=mode)\n\n    def set_data(self, path, data, *, _mode=0o666):\n        \"\"\"Write bytes data to a file.\"\"\"\n        parent, filename = _path_split(path)\n        path_parts = []\n        # Figure out what directories are missing.\n        while parent and not _path_isdir(parent):\n            parent, part = _path_split(parent)\n            path_parts.append(part)\n        # Create needed directories.\n        for part in reversed(path_parts):\n            parent = _path_join(parent, part)\n            try:\n                _os.mkdir(parent)\n            except FileExistsError:\n                # Probably another Python process already created the dir.\n                continue\n            except OSError as exc:\n                # Could be a permission error, read-only filesystem: just forget\n                # about writing the data.\n                _bootstrap._verbose_message('could not create {!r}: {!r}',\n                                            parent, exc)\n                return\n        try:\n            _write_atomic(path, data, _mode)\n            _bootstrap._verbose_message('created {!r}', path)\n        except OSError as exc:\n            # Same as above: just don't write the bytecode.\n            _bootstrap._verbose_message('could not create {!r}: {!r}', path,\n                                        exc)\n\n\nclass SourcelessFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader which handles sourceless file imports.\"\"\"\n\n    def get_code(self, fullname):\n        path = self.get_filename(fullname)\n        data = self.get_data(path)\n        # Call _classify_pyc to do basic validation of the pyc but ignore the\n        # result. There's no source to check against.\n        exc_details = {\n            'name': fullname,\n            'path': path,\n        }\n        _classify_pyc(data, fullname, exc_details)\n        return _compile_bytecode(\n            memoryview(data)[16:],\n            name=fullname,\n            bytecode_path=path,\n        )\n\n    def get_source(self, fullname):\n        \"\"\"Return None as there is no source code.\"\"\"\n        return None\n\n\nclass ExtensionFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader for extension modules.\n\n    The constructor is designed to work with FileFinder.\n\n    \"\"\"\n\n    def __init__(self, name, path):\n        self.name = name\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    def create_module(self, spec):\n        \"\"\"Create an uninitialized extension module\"\"\"\n        module = _bootstrap._call_with_frames_removed(\n            _imp.create_dynamic, spec)\n        _bootstrap._verbose_message('extension module {!r} loaded from {!r}',\n                         spec.name, self.path)\n        return module\n\n    def exec_module(self, module):\n        \"\"\"Initialize an extension module\"\"\"\n        _bootstrap._call_with_frames_removed(_imp.exec_dynamic, module)\n        _bootstrap._verbose_message('extension module {!r} executed from {!r}',\n                         self.name, self.path)\n\n    def is_package(self, fullname):\n        \"\"\"Return True if the extension module is a package.\"\"\"\n        file_name = _path_split(self.path)[1]\n        return any(file_name == '__init__' + suffix\n                   for suffix in EXTENSION_SUFFIXES)\n\n    def get_code(self, fullname):\n        \"\"\"Return None as an extension module cannot create a code object.\"\"\"\n        return None\n\n    def get_source(self, fullname):\n        \"\"\"Return None as extension modules have no source code.\"\"\"\n        return None\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n\nclass _NamespacePath:\n    \"\"\"Represents a namespace package's path.  It uses the module name\n    to find its parent module, and from there it looks up the parent's\n    __path__.  When this changes, the module's own path is recomputed,\n    using path_finder.  For top-level modules, the parent module's path\n    is sys.path.\"\"\"\n\n    # When invalidate_caches() is called, this epoch is incremented\n    # https://bugs.python.org/issue45703\n    _epoch = 0\n\n    def __init__(self, name, path, path_finder):\n        self._name = name\n        self._path = path\n        self._last_parent_path = tuple(self._get_parent_path())\n        self._last_epoch = self._epoch\n        self._path_finder = path_finder\n\n    def _find_parent_path_names(self):\n        \"\"\"Returns a tuple of (parent-module-name, parent-path-attr-name)\"\"\"\n        parent, dot, me = self._name.rpartition('.')\n        if dot == '':\n            # This is a top-level module. sys.path contains the parent path.\n            return 'sys', 'path'\n        # Not a top-level module. parent-module.__path__ contains the\n        #  parent path.\n        return parent, '__path__'\n\n    def _get_parent_path(self):\n        parent_module_name, path_attr_name = self._find_parent_path_names()\n        return getattr(sys.modules[parent_module_name], path_attr_name)\n\n    def _recalculate(self):\n        # If the parent's path has changed, recalculate _path\n        parent_path = tuple(self._get_parent_path()) # Make a copy\n        if parent_path != self._last_parent_path or self._epoch != self._last_epoch:\n            spec = self._path_finder(self._name, parent_path)\n            # Note that no changes are made if a loader is returned, but we\n            #  do remember the new parent path\n            if spec is not None and spec.loader is None:\n                if spec.submodule_search_locations:\n                    self._path = spec.submodule_search_locations\n            self._last_parent_path = parent_path     # Save the copy\n            self._last_epoch = self._epoch\n        return self._path\n\n    def __iter__(self):\n        return iter(self._recalculate())\n\n    def __getitem__(self, index):\n        return self._recalculate()[index]\n\n    def __setitem__(self, index, path):\n        self._path[index] = path\n\n    def __len__(self):\n        return len(self._recalculate())\n\n    def __repr__(self):\n        return f'_NamespacePath({self._path!r})'\n\n    def __contains__(self, item):\n        return item in self._recalculate()\n\n    def append(self, item):\n        self._path.append(item)\n\n\n# This class is actually exposed publicly in a namespace package's __loader__\n# attribute, so it should be available through a non-private name.\n# https://github.com/python/cpython/issues/92054\nclass NamespaceLoader:\n    def __init__(self, name, path, path_finder):\n        self._path = _NamespacePath(name, path, path_finder)\n\n    def is_package(self, fullname):\n        return True\n\n    def get_source(self, fullname):\n        return ''\n\n    def get_code(self, fullname):\n        return compile('', '<string>', 'exec', dont_inherit=True)\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        pass\n\n    def load_module(self, fullname):\n        \"\"\"Load a namespace module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The import system never calls this method.\n        _bootstrap._verbose_message('namespace module loaded with path {!r}',\n                                    self._path)\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n    def get_resource_reader(self, module):\n        from importlib.readers import NamespaceReader\n        return NamespaceReader(self._path)\n\n\n# We use this exclusively in module_from_spec() for backward-compatibility.\n_NamespaceLoader = NamespaceLoader\n\n\n# Finders #####################################################################\n\nclass PathFinder:\n\n    \"\"\"Meta path finder for sys.path and package __path__ attributes.\"\"\"\n\n    @staticmethod\n    def invalidate_caches():\n        \"\"\"Call the invalidate_caches() method on all path entry finders\n        stored in sys.path_importer_caches (where implemented).\"\"\"\n        for name, finder in list(sys.path_importer_cache.items()):\n            # Drop entry if finder name is a relative path. The current\n            # working directory may have changed.\n            if finder is None or not _path_isabs(name):\n                del sys.path_importer_cache[name]\n            elif hasattr(finder, 'invalidate_caches'):\n                finder.invalidate_caches()\n        # Also invalidate the caches of _NamespacePaths\n        # https://bugs.python.org/issue45703\n        _NamespacePath._epoch += 1\n\n        from importlib.metadata import MetadataPathFinder\n        MetadataPathFinder.invalidate_caches()\n\n    @staticmethod\n    def _path_hooks(path):\n        \"\"\"Search sys.path_hooks for a finder for 'path'.\"\"\"\n        if sys.path_hooks is not None and not sys.path_hooks:\n            _warnings.warn('sys.path_hooks is empty', ImportWarning)\n        for hook in sys.path_hooks:\n            try:\n                return hook(path)\n            except ImportError:\n                continue\n        else:\n            return None\n\n    @classmethod\n    def _path_importer_cache(cls, path):\n        \"\"\"Get the finder for the path entry from sys.path_importer_cache.\n\n        If the path entry is not in the cache, find the appropriate finder\n        and cache it. If no finder is available, store None.\n\n        \"\"\"\n        if path == '':\n            try:\n                path = _os.getcwd()\n            except FileNotFoundError:\n                # Don't cache the failure as the cwd can easily change to\n                # a valid directory later on.\n                return None\n        try:\n            finder = sys.path_importer_cache[path]\n        except KeyError:\n            finder = cls._path_hooks(path)\n            sys.path_importer_cache[path] = finder\n        return finder\n\n    @classmethod\n    def _get_spec(cls, fullname, path, target=None):\n        \"\"\"Find the loader or namespace_path for this module/package name.\"\"\"\n        # If this ends up being a namespace package, namespace_path is\n        #  the list of paths that will become its __path__\n        namespace_path = []\n        for entry in path:\n            if not isinstance(entry, str):\n                continue\n            finder = cls._path_importer_cache(entry)\n            if finder is not None:\n                spec = finder.find_spec(fullname, target)\n                if spec is None:\n                    continue\n                if spec.loader is not None:\n                    return spec\n                portions = spec.submodule_search_locations\n                if portions is None:\n                    raise ImportError('spec missing loader')\n                # This is possibly part of a namespace package.\n                #  Remember these path entries (if any) for when we\n                #  create a namespace package, and continue iterating\n                #  on path.\n                namespace_path.extend(portions)\n        else:\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = namespace_path\n            return spec\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        \"\"\"Try to find a spec for 'fullname' on sys.path or 'path'.\n\n        The search is based on sys.path_hooks and sys.path_importer_cache.\n        \"\"\"\n        if path is None:\n            path = sys.path\n        spec = cls._get_spec(fullname, path, target)\n        if spec is None:\n            return None\n        elif spec.loader is None:\n            namespace_path = spec.submodule_search_locations\n            if namespace_path:\n                # We found at least one namespace path.  Return a spec which\n                # can create the namespace package.\n                spec.origin = None\n                spec.submodule_search_locations = _NamespacePath(fullname, namespace_path, cls._get_spec)\n                return spec\n            else:\n                return None\n        else:\n            return spec\n\n    @staticmethod\n    def find_distributions(*args, **kwargs):\n        \"\"\"\n        Find distributions.\n\n        Return an iterable of all Distribution instances capable of\n        loading the metadata for packages matching ``context.name``\n        (or all names if ``None`` indicated) along the paths in the list\n        of directories ``context.path``.\n        \"\"\"\n        from importlib.metadata import MetadataPathFinder\n        return MetadataPathFinder.find_distributions(*args, **kwargs)\n\n\nclass FileFinder:\n\n    \"\"\"File-based finder.\n\n    Interactions with the file system are cached for performance, being\n    refreshed when the directory the finder is handling has been modified.\n\n    \"\"\"\n\n    def __init__(self, path, *loader_details):\n        \"\"\"Initialize with the path to search on and a variable number of\n        2-tuples containing the loader and the file suffixes the loader\n        recognizes.\"\"\"\n        loaders = []\n        for loader, suffixes in loader_details:\n            loaders.extend((suffix, loader) for suffix in suffixes)\n        self._loaders = loaders\n        # Base (directory) path\n        if not path or path == '.':\n            self.path = _os.getcwd()\n        else:\n            self.path = _path_abspath(path)\n        self._path_mtime = -1\n        self._path_cache = set()\n        self._relaxed_path_cache = set()\n\n    def invalidate_caches(self):\n        \"\"\"Invalidate the directory mtime.\"\"\"\n        self._path_mtime = -1\n\n    def _get_spec(self, loader_class, fullname, path, smsl, target):\n        loader = loader_class(fullname, path)\n        return spec_from_file_location(fullname, path, loader=loader,\n                                       submodule_search_locations=smsl)\n\n    def find_spec(self, fullname, target=None):\n        \"\"\"Try to find a spec for the specified module.\n\n        Returns the matching spec, or None if not found.\n        \"\"\"\n        is_namespace = False\n        tail_module = fullname.rpartition('.')[2]\n        try:\n            mtime = _path_stat(self.path or _os.getcwd()).st_mtime\n        except OSError:\n            mtime = -1\n        if mtime != self._path_mtime:\n            self._fill_cache()\n            self._path_mtime = mtime\n        # tail_module keeps the original casing, for __file__ and friends\n        if _relax_case():\n            cache = self._relaxed_path_cache\n            cache_module = tail_module.lower()\n        else:\n            cache = self._path_cache\n            cache_module = tail_module\n        # Check if the module is the name of a directory (and thus a package).\n        if cache_module in cache:\n            base_path = _path_join(self.path, tail_module)\n            for suffix, loader_class in self._loaders:\n                init_filename = '__init__' + suffix\n                full_path = _path_join(base_path, init_filename)\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path, [base_path], target)\n            else:\n                # If a namespace package, return the path if we don't\n                #  find a module in the next section.\n                is_namespace = _path_isdir(base_path)\n        # Check for a file w/ a proper suffix exists.\n        for suffix, loader_class in self._loaders:\n            try:\n                full_path = _path_join(self.path, tail_module + suffix)\n            except ValueError:\n                return None\n            _bootstrap._verbose_message('trying {}', full_path, verbosity=2)\n            if cache_module + suffix in cache:\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path,\n                                          None, target)\n        if is_namespace:\n            _bootstrap._verbose_message('possible namespace for {}', base_path)\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = [base_path]\n            return spec\n        return None\n\n    def _fill_cache(self):\n        \"\"\"Fill the cache of potential modules and packages for this directory.\"\"\"\n        path = self.path\n        try:\n            contents = _os.listdir(path or _os.getcwd())\n        except (FileNotFoundError, PermissionError, NotADirectoryError):\n            # Directory has either been removed, turned into a file, or made\n            # unreadable.\n            contents = []\n        # We store two cached versions, to handle runtime changes of the\n        # PYTHONCASEOK environment variable.\n        if not sys.platform.startswith('win'):\n            self._path_cache = set(contents)\n        else:\n            # Windows users can import modules with case-insensitive file\n            # suffixes (for legacy reasons). Make the suffix lowercase here\n            # so it's done once instead of for every import. This is safe as\n            # the specified suffixes to check against are always specified in a\n            # case-sensitive manner.\n            lower_suffix_contents = set()\n            for item in contents:\n                name, dot, suffix = item.partition('.')\n                if dot:\n                    new_name = f'{name}.{suffix.lower()}'\n                else:\n                    new_name = name\n                lower_suffix_contents.add(new_name)\n            self._path_cache = lower_suffix_contents\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n            self._relaxed_path_cache = {fn.lower() for fn in contents}\n\n    @classmethod\n    def path_hook(cls, *loader_details):\n        \"\"\"A class method which returns a closure to use on sys.path_hook\n        which will return an instance using the specified loaders and the path\n        called on the closure.\n\n        If the path called on the closure is not a directory, ImportError is\n        raised.\n\n        \"\"\"\n        def path_hook_for_FileFinder(path):\n            \"\"\"Path hook for importlib.machinery.FileFinder.\"\"\"\n            if not _path_isdir(path):\n                raise ImportError('only directories are supported', path=path)\n            return cls(path, *loader_details)\n\n        return path_hook_for_FileFinder\n\n    def __repr__(self):\n        return f'FileFinder({self.path!r})'\n\n\n# Import setup ###############################################################\n\ndef _fix_up_module(ns, name, pathname, cpathname=None):\n    # This function is used by PyImport_ExecCodeModuleObject().\n    loader = ns.get('__loader__')\n    spec = ns.get('__spec__')\n    if not loader:\n        if spec:\n            loader = spec.loader\n        elif pathname == cpathname:\n            loader = SourcelessFileLoader(name, pathname)\n        else:\n            loader = SourceFileLoader(name, pathname)\n    if not spec:\n        spec = spec_from_file_location(name, pathname, loader=loader)\n        if cpathname:\n            spec.cached = _path_abspath(cpathname)\n    try:\n        ns['__spec__'] = spec\n        ns['__loader__'] = loader\n        ns['__file__'] = pathname\n        ns['__cached__'] = cpathname\n    except Exception:\n        # Not important enough to report.\n        pass\n\n\ndef _get_supported_file_loaders():\n    \"\"\"Returns a list of file-based module loaders.\n\n    Each item is a tuple (loader, suffixes).\n    \"\"\"\n    extensions = ExtensionFileLoader, _imp.extension_suffixes()\n    source = SourceFileLoader, SOURCE_SUFFIXES\n    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES\n    return [extensions, source, bytecode]\n\n\ndef _set_bootstrap_module(_bootstrap_module):\n    global _bootstrap\n    _bootstrap = _bootstrap_module\n\n\ndef _install(_bootstrap_module):\n    \"\"\"Install the path-based import components.\"\"\"\n    _set_bootstrap_module(_bootstrap_module)\n    supported_loaders = _get_supported_file_loaders()\n    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])\n    sys.meta_path.append(PathFinder)\n", 1749], "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport asyncio\nimport atexit\nimport gc\nimport importlib\nimport inspect\nimport json\nimport multiprocessing\nimport os\nimport signal\nimport socket\nimport tempfile\nimport uuid\nfrom argparse import Namespace\nfrom collections.abc import AsyncIterator, Awaitable\nfrom contextlib import asynccontextmanager\nfrom functools import partial\nfrom http import HTTPStatus\nfrom typing import Annotated, Any, Callable, Optional\n\nimport prometheus_client\nimport pydantic\nimport regex as re\nimport uvloop\nfrom fastapi import APIRouter, Depends, FastAPI, Form, HTTPException, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, Response, StreamingResponse\nfrom prometheus_client import make_asgi_app\nfrom prometheus_fastapi_instrumentator import Instrumentator\nfrom starlette.concurrency import iterate_in_threadpool\nfrom starlette.datastructures import URL, Headers, MutableHeaders, State\nfrom starlette.routing import Mount\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import VllmConfig\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine  # type: ignore\nfrom vllm.engine.multiprocessing.client import MQLLMEngineClient\nfrom vllm.engine.multiprocessing.engine import run_mp_engine\nfrom vllm.engine.protocol import EngineClient\nfrom vllm.entrypoints.chat_utils import (load_chat_template,\n                                         resolve_hf_chat_template,\n                                         resolve_mistral_chat_template)\nfrom vllm.entrypoints.launcher import serve_http\nfrom vllm.entrypoints.logger import RequestLogger\nfrom vllm.entrypoints.openai.cli_args import (make_arg_parser,\n                                              validate_parsed_serve_args)\n# yapf conflicts with isort for this block\n# yapf: disable\nfrom vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\n                                              ChatCompletionResponse,\n                                              ClassificationRequest,\n                                              ClassificationResponse,\n                                              CompletionRequest,\n                                              CompletionResponse,\n                                              DetokenizeRequest,\n                                              DetokenizeResponse,\n                                              EmbeddingRequest,\n                                              EmbeddingResponse, ErrorResponse,\n                                              LoadLoRAAdapterRequest,\n                                              PoolingRequest, PoolingResponse,\n                                              RerankRequest, RerankResponse,\n                                              ResponsesRequest,\n                                              ResponsesResponse, ScoreRequest,\n                                              ScoreResponse, TokenizeRequest,\n                                              TokenizeResponse,\n                                              TranscriptionRequest,\n                                              TranscriptionResponse,\n                                              TranslationRequest,\n                                              TranslationResponse,\n                                              UnloadLoRAAdapterRequest)\n# yapf: enable\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom vllm.entrypoints.openai.serving_classification import (\n    ServingClassification)\nfrom vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom vllm.entrypoints.openai.serving_embedding import OpenAIServingEmbedding\nfrom vllm.entrypoints.openai.serving_engine import OpenAIServing\nfrom vllm.entrypoints.openai.serving_models import (BaseModelPath,\n                                                    LoRAModulePath,\n                                                    OpenAIServingModels)\nfrom vllm.entrypoints.openai.serving_pooling import OpenAIServingPooling\nfrom vllm.entrypoints.openai.serving_responses import OpenAIServingResponses\nfrom vllm.entrypoints.openai.serving_score import ServingScores\nfrom vllm.entrypoints.openai.serving_tokenization import (\n    OpenAIServingTokenization)\nfrom vllm.entrypoints.openai.serving_transcription import (\n    OpenAIServingTranscription, OpenAIServingTranslation)\nfrom vllm.entrypoints.openai.tool_parsers import ToolParserManager\nfrom vllm.entrypoints.openai.tool_server import (DemoToolServer, MCPToolServer,\n                                                 ToolServer)\nfrom vllm.entrypoints.utils import (cli_env_setup, load_aware_call,\n                                    log_non_default_args, with_cancellation)\nfrom vllm.logger import init_logger\nfrom vllm.reasoning import ReasoningParserManager\nfrom vllm.transformers_utils.config import (\n    maybe_register_config_serialize_by_value)\nfrom vllm.transformers_utils.tokenizer import MistralTokenizer\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import (Device, FlexibleArgumentParser, decorate_logs,\n                        get_open_zmq_ipc_path, is_valid_ipv6_address,\n                        set_ulimit)\nfrom vllm.v1.metrics.prometheus import get_prometheus_registry\nfrom vllm.version import __version__ as VLLM_VERSION\n\nprometheus_multiproc_dir: tempfile.TemporaryDirectory\n\n# Cannot use __name__ (https://github.com/vllm-project/vllm/pull/4765)\nlogger = init_logger('vllm.entrypoints.openai.api_server')\n\n_running_tasks: set[asyncio.Task] = set()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        if app.state.log_stats:\n            engine_client: EngineClient = app.state.engine_client\n\n            async def _force_log():\n                while True:\n                    await asyncio.sleep(10.)\n                    await engine_client.do_log_stats()\n\n            task = asyncio.create_task(_force_log())\n            _running_tasks.add(task)\n            task.add_done_callback(_running_tasks.remove)\n        else:\n            task = None\n\n        # Mark the startup heap as static so that it's ignored by GC.\n        # Reduces pause times of oldest generation collections.\n        gc.collect()\n        gc.freeze()\n        try:\n            yield\n        finally:\n            if task is not None:\n                task.cancel()\n    finally:\n        # Ensure app state including engine ref is gc'd\n        del app.state\n\n\n@asynccontextmanager\nasync def build_async_engine_client(\n    args: Namespace,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: Optional[bool] = None,\n    client_config: Optional[dict[str, Any]] = None,\n) -> AsyncIterator[EngineClient]:\n\n    # Context manager to handle engine_client lifecycle\n    # Ensures everything is shutdown and cleaned up on error/exit\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n\n    if disable_frontend_multiprocessing is None:\n        disable_frontend_multiprocessing = bool(\n            args.disable_frontend_multiprocessing)\n\n    async with build_async_engine_client_from_engine_args(\n            engine_args,\n            usage_context=usage_context,\n            disable_frontend_multiprocessing=disable_frontend_multiprocessing,\n            client_config=client_config,\n    ) as engine:\n        yield engine\n\n\n@asynccontextmanager\nasync def build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool = False,\n    client_config: Optional[dict[str, Any]] = None,\n) -> AsyncIterator[EngineClient]:\n    \"\"\"\n    Create EngineClient, either:\n        - in-process using the AsyncLLMEngine Directly\n        - multiprocess using AsyncLLMEngine RPC\n\n    Returns the Client or None if the creation failed.\n    \"\"\"\n\n    # Create the EngineConfig (determines if we can use V1).\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\n    # V1 AsyncLLM.\n    if envs.VLLM_USE_V1:\n        if disable_frontend_multiprocessing:\n            logger.warning(\n                \"V1 is enabled, but got --disable-frontend-multiprocessing. \"\n                \"To disable frontend multiprocessing, set VLLM_USE_V1=0.\")\n\n        from vllm.v1.engine.async_llm import AsyncLLM\n        async_llm: Optional[AsyncLLM] = None\n        client_count = client_config.pop(\n            \"client_count\") if client_config else 1\n        client_index = client_config.pop(\n            \"client_index\") if client_config else 0\n        try:\n            async_llm = AsyncLLM.from_vllm_config(\n                vllm_config=vllm_config,\n                usage_context=usage_context,\n                enable_log_requests=engine_args.enable_log_requests,\n                disable_log_stats=engine_args.disable_log_stats,\n                client_addresses=client_config,\n                client_count=client_count,\n                client_index=client_index)\n\n            # Don't keep the dummy data in memory\n            await async_llm.reset_mm_cache()\n\n            yield async_llm\n        finally:\n            if async_llm:\n                async_llm.shutdown()\n\n    # V0 AsyncLLM.\n    elif (MQLLMEngineClient.is_unsupported_config(vllm_config)\n          or disable_frontend_multiprocessing):\n\n        engine_client: Optional[EngineClient] = None\n        try:\n            engine_client = AsyncLLMEngine.from_vllm_config(\n                vllm_config=vllm_config,\n                usage_context=usage_context,\n                enable_log_requests=engine_args.enable_log_requests,\n                disable_log_stats=engine_args.disable_log_stats)\n            yield engine_client\n        finally:\n            if engine_client and hasattr(engine_client, \"shutdown\"):\n                engine_client.shutdown()\n\n    # V0MQLLMEngine.\n    else:\n        if \"PROMETHEUS_MULTIPROC_DIR\" not in os.environ:\n            # Make TemporaryDirectory for prometheus multiprocessing\n            # Note: global TemporaryDirectory will be automatically\n            #   cleaned up upon exit.\n            global prometheus_multiproc_dir\n            prometheus_multiproc_dir = tempfile.TemporaryDirectory()\n            os.environ[\n                \"PROMETHEUS_MULTIPROC_DIR\"] = prometheus_multiproc_dir.name\n        else:\n            logger.warning(\n                \"Found PROMETHEUS_MULTIPROC_DIR was set by user. \"\n                \"This directory must be wiped between vLLM runs or \"\n                \"you will find inaccurate metrics. Unset the variable \"\n                \"and vLLM will properly handle cleanup.\")\n\n        # Select random path for IPC.\n        ipc_path = get_open_zmq_ipc_path()\n        logger.debug(\"Multiprocessing frontend to use %s for IPC Path.\",\n                     ipc_path)\n\n        # Start RPCServer in separate process (holds the LLMEngine).\n        # the current process might have CUDA context,\n        # so we need to spawn a new process\n        context = multiprocessing.get_context(\"spawn\")\n\n        # Ensure we can serialize transformer config before spawning\n        maybe_register_config_serialize_by_value()\n\n        # The Process can raise an exception during startup, which may\n        # not actually result in an exitcode being reported. As a result\n        # we use a shared variable to communicate the information.\n        engine_alive = multiprocessing.Value('b', True, lock=False)\n        engine_process = context.Process(\n            target=run_mp_engine,\n            args=(vllm_config, UsageContext.OPENAI_API_SERVER, ipc_path,\n                  engine_args.disable_log_stats,\n                  engine_args.enable_log_requests, engine_alive))\n        engine_process.start()\n        engine_pid = engine_process.pid\n        assert engine_pid is not None, \"Engine process failed to start.\"\n        logger.info(\"Started engine process with PID %d\", engine_pid)\n\n        def _cleanup_ipc_path():\n            socket_path = ipc_path.replace(\"ipc://\", \"\")\n            if os.path.exists(socket_path):\n                os.remove(socket_path)\n\n        # Ensure we clean up the local IPC socket file on exit.\n        atexit.register(_cleanup_ipc_path)\n\n        # Build RPCClient, which conforms to EngineClient Protocol.\n        build_client = partial(MQLLMEngineClient, ipc_path, vllm_config,\n                               engine_pid)\n        mq_engine_client = await asyncio.get_running_loop().run_in_executor(\n            None, build_client)\n        mq_engine_client.engine_proc = engine_process\n        try:\n            while True:\n                try:\n                    await mq_engine_client.setup()\n                    break\n                except TimeoutError:\n                    if (not engine_process.is_alive()\n                            or not engine_alive.value):\n                        raise RuntimeError(\n                            \"Engine process failed to start. See stack \"\n                            \"trace for the root cause.\") from None\n\n            yield mq_engine_client  # type: ignore[misc]\n        finally:\n            # Ensure rpc server process was terminated\n            engine_process.terminate()\n\n            # Close all open connections to the backend\n            mq_engine_client.close()\n\n            # Wait for engine process to join\n            engine_process.join(4)\n            if engine_process.exitcode is None:\n                # Kill if taking longer than 5 seconds to stop\n                engine_process.kill()\n\n            # Lazy import for prometheus multiprocessing.\n            # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n            # before prometheus_client is imported.\n            # See https://prometheus.github.io/client_python/multiprocess/\n            from prometheus_client import multiprocess\n            multiprocess.mark_process_dead(engine_process.pid)\n\n\nasync def validate_json_request(raw_request: Request):\n    content_type = raw_request.headers.get(\"content-type\", \"\").lower()\n    media_type = content_type.split(\";\", maxsplit=1)[0]\n    if media_type != \"application/json\":\n        raise RequestValidationError(errors=[\n            \"Unsupported Media Type: Only 'application/json' is allowed\"\n        ])\n\n\nrouter = APIRouter()\n\n\nclass PrometheusResponse(Response):\n    media_type = prometheus_client.CONTENT_TYPE_LATEST\n\n\ndef mount_metrics(app: FastAPI):\n    \"\"\"Mount prometheus metrics to a FastAPI app.\"\"\"\n\n    registry = get_prometheus_registry()\n\n    # `response_class=PrometheusResponse` is needed to return an HTTP response\n    # with header \"Content-Type: text/plain; version=0.0.4; charset=utf-8\"\n    # instead of the default \"application/json\" which is incorrect.\n    # See https://github.com/trallnag/prometheus-fastapi-instrumentator/issues/163#issue-1296092364\n    Instrumentator(\n        excluded_handlers=[\n            \"/metrics\",\n            \"/health\",\n            \"/load\",\n            \"/ping\",\n            \"/version\",\n            \"/server_info\",\n        ],\n        registry=registry,\n    ).add().instrument(app).expose(app, response_class=PrometheusResponse)\n\n    # Add prometheus asgi middleware to route /metrics requests\n    metrics_route = Mount(\"/metrics\", make_asgi_app(registry=registry))\n\n    # Workaround for 307 Redirect for /metrics\n    metrics_route.path_regex = re.compile(\"^/metrics(?P<path>.*)$\")\n    app.routes.append(metrics_route)\n\n\ndef base(request: Request) -> OpenAIServing:\n    # Reuse the existing instance\n    return tokenization(request)\n\n\ndef models(request: Request) -> OpenAIServingModels:\n    return request.app.state.openai_serving_models\n\n\ndef responses(request: Request) -> Optional[OpenAIServingResponses]:\n    return request.app.state.openai_serving_responses\n\n\ndef chat(request: Request) -> Optional[OpenAIServingChat]:\n    return request.app.state.openai_serving_chat\n\n\ndef completion(request: Request) -> Optional[OpenAIServingCompletion]:\n    return request.app.state.openai_serving_completion\n\n\ndef pooling(request: Request) -> Optional[OpenAIServingPooling]:\n    return request.app.state.openai_serving_pooling\n\n\ndef embedding(request: Request) -> Optional[OpenAIServingEmbedding]:\n    return request.app.state.openai_serving_embedding\n\n\ndef score(request: Request) -> Optional[ServingScores]:\n    return request.app.state.openai_serving_scores\n\n\ndef classify(request: Request) -> Optional[ServingClassification]:\n    return request.app.state.openai_serving_classification\n\n\ndef rerank(request: Request) -> Optional[ServingScores]:\n    return request.app.state.openai_serving_scores\n\n\ndef tokenization(request: Request) -> OpenAIServingTokenization:\n    return request.app.state.openai_serving_tokenization\n\n\ndef transcription(request: Request) -> OpenAIServingTranscription:\n    return request.app.state.openai_serving_transcription\n\n\ndef translation(request: Request) -> OpenAIServingTranslation:\n    return request.app.state.openai_serving_translation\n\n\ndef engine_client(request: Request) -> EngineClient:\n    return request.app.state.engine_client\n\n\n@router.get(\"/health\", response_class=Response)\nasync def health(raw_request: Request) -> Response:\n    \"\"\"Health check.\"\"\"\n    await engine_client(raw_request).check_health()\n    return Response(status_code=200)\n\n\n@router.get(\"/load\")\nasync def get_server_load_metrics(request: Request):\n    # This endpoint returns the current server load metrics.\n    # It tracks requests utilizing the GPU from the following routes:\n    # - /v1/chat/completions\n    # - /v1/completions\n    # - /v1/audio/transcriptions\n    # - /v1/audio/translations\n    # - /v1/embeddings\n    # - /pooling\n    # - /classify\n    # - /score\n    # - /v1/score\n    # - /rerank\n    # - /v1/rerank\n    # - /v2/rerank\n    return JSONResponse(\n        content={'server_load': request.app.state.server_load_metrics})\n\n\n@router.get(\"/ping\", response_class=Response)\n@router.post(\"/ping\", response_class=Response)\nasync def ping(raw_request: Request) -> Response:\n    \"\"\"Ping check. Endpoint required for SageMaker\"\"\"\n    return await health(raw_request)\n\n\n@router.post(\"/tokenize\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_FOUND.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_IMPLEMENTED.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\nasync def tokenize(request: TokenizeRequest, raw_request: Request):\n    handler = tokenization(raw_request)\n\n    try:\n        generator = await handler.create_tokenize(request, raw_request)\n    except NotImplementedError as e:\n        raise HTTPException(status_code=HTTPStatus.NOT_IMPLEMENTED.value,\n                            detail=str(e)) from e\n    except Exception as e:\n        raise HTTPException(status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value,\n                            detail=str(e)) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, TokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/detokenize\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_FOUND.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\nasync def detokenize(request: DetokenizeRequest, raw_request: Request):\n    handler = tokenization(raw_request)\n\n    try:\n        generator = await handler.create_detokenize(request, raw_request)\n    except OverflowError as e:\n        raise RequestValidationError(errors=[str(e)]) from e\n    except Exception as e:\n        raise HTTPException(status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value,\n                            detail=str(e)) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, DetokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\ndef maybe_register_tokenizer_info_endpoint(args):\n    \"\"\"Conditionally register the tokenizer info endpoint if enabled.\"\"\"\n    if getattr(args, 'enable_tokenizer_info_endpoint', False):\n\n        @router.get(\"/tokenizer_info\")\n        async def get_tokenizer_info(raw_request: Request):\n            \"\"\"Get comprehensive tokenizer information.\"\"\"\n            result = await tokenization(raw_request).get_tokenizer_info()\n            return JSONResponse(content=result.model_dump(),\n                                status_code=result.code if isinstance(\n                                    result, ErrorResponse) else 200)\n\n\n@router.get(\"/v1/models\")\nasync def show_available_models(raw_request: Request):\n    handler = models(raw_request)\n\n    models_ = await handler.show_available_models()\n    return JSONResponse(content=models_.model_dump())\n\n\n@router.get(\"/version\")\nasync def show_version():\n    ver = {\"version\": VLLM_VERSION}\n    return JSONResponse(content=ver)\n\n\n@router.post(\"/v1/responses\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"content\": {\n                         \"text/event-stream\": {}\n                     }\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_FOUND.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\nasync def create_responses(request: ResponsesRequest, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\")\n\n    generator = await handler.create_responses(request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, ResponsesResponse):\n        return JSONResponse(content=generator.model_dump())\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.get(\"/v1/responses/{response_id}\")\nasync def retrieve_responses(response_id: str, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\")\n\n    response = await handler.retrieve_responses(response_id)\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(content=response.model_dump(),\n                            status_code=response.code)\n    return JSONResponse(content=response.model_dump())\n\n\n@router.post(\"/v1/responses/{response_id}/cancel\")\nasync def cancel_responses(response_id: str, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\")\n\n    response = await handler.cancel_responses(response_id)\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(content=response.model_dump(),\n                            status_code=response.code)\n    return JSONResponse(content=response.model_dump())\n\n\n@router.post(\"/v1/chat/completions\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"content\": {\n                         \"text/event-stream\": {}\n                     }\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_FOUND.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 }\n             })\n@with_cancellation\n@load_aware_call\nasync def create_chat_completion(request: ChatCompletionRequest,\n                                 raw_request: Request):\n    handler = chat(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Chat Completions API\")\n\n    generator = await handler.create_chat_completion(request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, ChatCompletionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/completions\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"content\": {\n                         \"text/event-stream\": {}\n                     }\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.NOT_FOUND.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_completion(request: CompletionRequest, raw_request: Request):\n    handler = completion(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Completions API\")\n\n    try:\n        generator = await handler.create_completion(request, raw_request)\n    except OverflowError as e:\n        raise HTTPException(status_code=HTTPStatus.BAD_REQUEST.value,\n                            detail=str(e)) from e\n    except Exception as e:\n        raise HTTPException(status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value,\n                            detail=str(e)) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, CompletionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/embeddings\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_embedding(request: EmbeddingRequest, raw_request: Request):\n    handler = embedding(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Embeddings API\")\n\n    generator = await handler.create_embedding(request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, EmbeddingResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/pooling\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_pooling(request: PoolingRequest, raw_request: Request):\n    handler = pooling(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Pooling API\")\n\n    generator = await handler.create_pooling(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, PoolingResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/classify\", dependencies=[Depends(validate_json_request)])\n@with_cancellation\n@load_aware_call\nasync def create_classify(request: ClassificationRequest,\n                          raw_request: Request):\n    handler = classify(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Classification API\")\n\n    generator = await handler.create_classify(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, ClassificationResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/score\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_score(request: ScoreRequest, raw_request: Request):\n    handler = score(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Score API\")\n\n    generator = await handler.create_score(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, ScoreResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/v1/score\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_score_v1(request: ScoreRequest, raw_request: Request):\n    logger.warning(\n        \"To indicate that Score API is not part of standard OpenAI API, we \"\n        \"have moved it to `/score`. Please update your client accordingly.\")\n\n    return await create_score(request, raw_request)\n\n\n@router.post(\"/v1/audio/transcriptions\",\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"content\": {\n                         \"text/event-stream\": {}\n                     }\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.UNPROCESSABLE_ENTITY.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_transcriptions(raw_request: Request,\n                                request: Annotated[TranscriptionRequest,\n                                                   Form()]):\n    handler = transcription(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Transcriptions API\")\n\n    audio_data = await request.file.read()\n    generator = await handler.create_transcription(audio_data, request,\n                                                   raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, TranscriptionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/audio/translations\",\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"content\": {\n                         \"text/event-stream\": {}\n                     }\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.UNPROCESSABLE_ENTITY.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def create_translations(request: Annotated[TranslationRequest,\n                                                 Form()],\n                              raw_request: Request):\n    handler = translation(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Translations API\")\n\n    audio_data = await request.file.read()\n    generator = await handler.create_translation(audio_data, request,\n                                                 raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, TranslationResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/rerank\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\n@load_aware_call\nasync def do_rerank(request: RerankRequest, raw_request: Request):\n    handler = rerank(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Rerank (Score) API\")\n    generator = await handler.do_rerank(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, RerankResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/v1/rerank\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\nasync def do_rerank_v1(request: RerankRequest, raw_request: Request):\n    logger.warning_once(\n        \"To indicate that the rerank API is not part of the standard OpenAI\"\n        \" API, we have located it at `/rerank`. Please update your client \"\n        \"accordingly. (Note: Conforms to JinaAI rerank API)\")\n\n    return await do_rerank(request, raw_request)\n\n\n@router.post(\"/v2/rerank\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\n@with_cancellation\nasync def do_rerank_v2(request: RerankRequest, raw_request: Request):\n    return await do_rerank(request, raw_request)\n\n\nif envs.VLLM_SERVER_DEV_MODE:\n    logger.warning(\"SECURITY WARNING: Development endpoints are enabled! \"\n                   \"This should NOT be used in production!\")\n\n    @router.get(\"/server_info\")\n    async def show_server_info(raw_request: Request):\n        server_info = {\"vllm_config\": str(raw_request.app.state.vllm_config)}\n        return JSONResponse(content=server_info)\n\n    @router.post(\"/reset_prefix_cache\")\n    async def reset_prefix_cache(raw_request: Request):\n        \"\"\"\n        Reset the prefix cache. Note that we currently do not check if the\n        prefix cache is successfully reset in the API server.\n        \"\"\"\n        device = None\n        device_str = raw_request.query_params.get(\"device\")\n        if device_str is not None:\n            device = Device[device_str.upper()]\n        logger.info(\"Resetting prefix cache with specific %s...\", str(device))\n        await engine_client(raw_request).reset_prefix_cache(device)\n        return Response(status_code=200)\n\n    @router.post(\"/sleep\")\n    async def sleep(raw_request: Request):\n        # get POST params\n        level = raw_request.query_params.get(\"level\", \"1\")\n        await engine_client(raw_request).sleep(int(level))\n        # FIXME: in v0 with frontend multiprocessing, the sleep command\n        # is sent but does not finish yet when we return a response.\n        return Response(status_code=200)\n\n    @router.post(\"/wake_up\")\n    async def wake_up(raw_request: Request):\n        tags = raw_request.query_params.getlist(\"tags\")\n        if tags == []:\n            # set to None to wake up all tags if no tags are provided\n            tags = None\n        logger.info(\"wake up the engine with tags: %s\", tags)\n        await engine_client(raw_request).wake_up(tags)\n        # FIXME: in v0 with frontend multiprocessing, the wake-up command\n        # is sent but does not finish yet when we return a response.\n        return Response(status_code=200)\n\n    @router.get(\"/is_sleeping\")\n    async def is_sleeping(raw_request: Request):\n        logger.info(\"check whether the engine is sleeping\")\n        is_sleeping = await engine_client(raw_request).is_sleeping()\n        return JSONResponse(content={\"is_sleeping\": is_sleeping})\n\n\n@router.post(\"/scale_elastic_ep\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.OK.value: {\n                     \"model\": dict\n                 },\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.REQUEST_TIMEOUT.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\nasync def scale_elastic_ep(raw_request: Request):\n    try:\n        body = await raw_request.json()\n    except json.JSONDecodeError as e:\n        raise HTTPException(status_code=400,\n                            detail=\"Invalid JSON format\") from e  # noqa: B904\n\n    new_data_parallel_size = body.get(\"new_data_parallel_size\")\n    drain_timeout = body.get(\"drain_timeout\", 120)  # Default 2 minutes\n\n    if new_data_parallel_size is None:\n        raise HTTPException(status_code=400,\n                            detail=\"new_data_parallel_size is required\")\n\n    if not isinstance(new_data_parallel_size,\n                      int) or new_data_parallel_size <= 0:\n        raise HTTPException(\n            status_code=400,\n            detail=\"new_data_parallel_size must be a positive integer\")\n\n    if not isinstance(drain_timeout, int) or drain_timeout <= 0:\n        raise HTTPException(status_code=400,\n                            detail=\"drain_timeout must be a positive integer\")\n\n    # Set scaling flag to prevent new requests\n    global _scaling_elastic_ep\n    _scaling_elastic_ep = True\n    client = engine_client(raw_request)\n    try:\n        await client.scale_elastic_ep(new_data_parallel_size, drain_timeout)\n        return JSONResponse({\n            \"message\":\n            f\"Scaled to {new_data_parallel_size} \"\n            \"data parallel engines\",\n        })\n    except TimeoutError as e:\n        raise HTTPException(status_code=408,\n                            detail=\"Scale failed due to request drain timeout \"\n                            f\"after {drain_timeout} seconds\") from e\n    except Exception as e:\n        logger.error(\"Scale failed: %s\", e)\n        raise HTTPException(status_code=500, detail=\"Scale failed\") from e\n    finally:\n        _scaling_elastic_ep = False\n\n\n@router.post(\"/is_scaling_elastic_ep\")\nasync def is_scaling_elastic_ep(raw_request: Request):\n    return JSONResponse({\"is_scaling_elastic_ep\": _scaling_elastic_ep})\n\n\n# TODO: RequestType = TypeForm[BaseModel] when recognized by type checkers\n# (requires typing_extensions >= 4.13)\nRequestType = Any\nGetHandlerFn = Callable[[Request], Optional[OpenAIServing]]\nEndpointFn = Callable[[RequestType, Request], Awaitable[Any]]\n\n# NOTE: Items defined earlier take higher priority\nINVOCATION_TYPES: list[tuple[RequestType, tuple[GetHandlerFn, EndpointFn]]] = [\n    (ChatCompletionRequest, (chat, create_chat_completion)),\n    (CompletionRequest, (completion, create_completion)),\n    (EmbeddingRequest, (embedding, create_embedding)),\n    (ClassificationRequest, (classify, create_classify)),\n    (ScoreRequest, (score, create_score)),\n    (RerankRequest, (rerank, do_rerank)),\n    (PoolingRequest, (pooling, create_pooling)),\n]\n\n# NOTE: Construct the TypeAdapters only once\nINVOCATION_VALIDATORS = [\n    (pydantic.TypeAdapter(request_type), (get_handler, endpoint))\n    for request_type, (get_handler, endpoint) in INVOCATION_TYPES\n]\n\n\n@router.post(\"/invocations\",\n             dependencies=[Depends(validate_json_request)],\n             responses={\n                 HTTPStatus.BAD_REQUEST.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.UNSUPPORTED_MEDIA_TYPE.value: {\n                     \"model\": ErrorResponse\n                 },\n                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n                     \"model\": ErrorResponse\n                 },\n             })\nasync def invocations(raw_request: Request):\n    \"\"\"For SageMaker, routes requests based on the request type.\"\"\"\n    try:\n        body = await raw_request.json()\n    except json.JSONDecodeError as e:\n        raise HTTPException(status_code=HTTPStatus.BAD_REQUEST.value,\n                            detail=f\"JSON decode error: {e}\") from e\n\n    valid_endpoints = [(validator, endpoint)\n                       for validator, (get_handler,\n                                       endpoint) in INVOCATION_VALIDATORS\n                       if get_handler(raw_request) is not None]\n\n    for request_validator, endpoint in valid_endpoints:\n        try:\n            request = request_validator.validate_python(body)\n        except pydantic.ValidationError:\n            continue\n\n        return await endpoint(request, raw_request)\n\n    type_names = [\n        t.__name__ if isinstance(t := validator._type, type) else str(t)\n        for validator, _ in valid_endpoints\n    ]\n    msg = (\"Cannot find suitable handler for request. \"\n           f\"Expected one of: {type_names}\")\n    res = base(raw_request).create_error_response(message=msg)\n    return JSONResponse(content=res.model_dump(), status_code=res.code)\n\n\nif envs.VLLM_TORCH_PROFILER_DIR:\n    logger.warning(\n        \"Torch Profiler is enabled in the API server. This should ONLY be \"\n        \"used for local development!\")\n\n    @router.post(\"/start_profile\")\n    async def start_profile(raw_request: Request):\n        logger.info(\"Starting profiler...\")\n        await engine_client(raw_request).start_profile()\n        logger.info(\"Profiler started.\")\n        return Response(status_code=200)\n\n    @router.post(\"/stop_profile\")\n    async def stop_profile(raw_request: Request):\n        logger.info(\"Stopping profiler...\")\n        await engine_client(raw_request).stop_profile()\n        logger.info(\"Profiler stopped.\")\n        return Response(status_code=200)\n\n\nif envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING:\n    logger.warning(\n        \"LoRA dynamic loading & unloading is enabled in the API server. \"\n        \"This should ONLY be used for local development!\")\n\n    @router.post(\"/v1/load_lora_adapter\",\n                 dependencies=[Depends(validate_json_request)])\n    async def load_lora_adapter(request: LoadLoRAAdapterRequest,\n                                raw_request: Request):\n        handler = models(raw_request)\n        response = await handler.load_lora_adapter(request)\n        if isinstance(response, ErrorResponse):\n            return JSONResponse(content=response.model_dump(),\n                                status_code=response.code)\n\n        return Response(status_code=200, content=response)\n\n    @router.post(\"/v1/unload_lora_adapter\",\n                 dependencies=[Depends(validate_json_request)])\n    async def unload_lora_adapter(request: UnloadLoRAAdapterRequest,\n                                  raw_request: Request):\n        handler = models(raw_request)\n        response = await handler.unload_lora_adapter(request)\n        if isinstance(response, ErrorResponse):\n            return JSONResponse(content=response.model_dump(),\n                                status_code=response.code)\n\n        return Response(status_code=200, content=response)\n\n\ndef load_log_config(log_config_file: Optional[str]) -> Optional[dict]:\n    if not log_config_file:\n        return None\n    try:\n        with open(log_config_file) as f:\n            return json.load(f)\n    except Exception as e:\n        logger.warning(\"Failed to load log config from file %s: error %s\",\n                       log_config_file, e)\n        return None\n\n\nclass AuthenticationMiddleware:\n    \"\"\"\n    Pure ASGI middleware that authenticates each request by checking\n    if the Authorization header exists and equals \"Bearer {api_key}\".\n\n    Notes\n    -----\n    There are two cases in which authentication is skipped:\n        1. The HTTP method is OPTIONS.\n        2. The request path doesn't start with /v1 (e.g. /health).\n    \"\"\"\n\n    def __init__(self, app: ASGIApp, tokens: list[str]) -> None:\n        self.app = app\n        self.api_tokens = {f\"Bearer {token}\" for token in tokens}\n\n    def __call__(self, scope: Scope, receive: Receive,\n                 send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\",\n                                 \"websocket\") or scope[\"method\"] == \"OPTIONS\":\n            # scope[\"type\"] can be \"lifespan\" or \"startup\" for example,\n            # in which case we don't need to do anything\n            return self.app(scope, receive, send)\n        root_path = scope.get(\"root_path\", \"\")\n        url_path = URL(scope=scope).path.removeprefix(root_path)\n        headers = Headers(scope=scope)\n        # Type narrow to satisfy mypy.\n        if url_path.startswith(\"/v1\") and headers.get(\n                \"Authorization\") not in self.api_tokens:\n            response = JSONResponse(content={\"error\": \"Unauthorized\"},\n                                    status_code=401)\n            return response(scope, receive, send)\n        return self.app(scope, receive, send)\n\n\nclass XRequestIdMiddleware:\n    \"\"\"\n    Middleware the set's the X-Request-Id header for each response\n    to a random uuid4 (hex) value if the header isn't already\n    present in the request, otherwise use the provided request id.\n    \"\"\"\n\n    def __init__(self, app: ASGIApp) -> None:\n        self.app = app\n\n    def __call__(self, scope: Scope, receive: Receive,\n                 send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            return self.app(scope, receive, send)\n\n        # Extract the request headers.\n        request_headers = Headers(scope=scope)\n\n        async def send_with_request_id(message: Message) -> None:\n            \"\"\"\n            Custom send function to mutate the response headers\n            and append X-Request-Id to it.\n            \"\"\"\n            if message[\"type\"] == \"http.response.start\":\n                response_headers = MutableHeaders(raw=message[\"headers\"])\n                request_id = request_headers.get(\"X-Request-Id\",\n                                                 uuid.uuid4().hex)\n                response_headers.append(\"X-Request-Id\", request_id)\n            await send(message)\n\n        return self.app(scope, receive, send_with_request_id)\n\n\n# Global variable to track scaling state\n_scaling_elastic_ep = False\n\n\nclass ScalingMiddleware:\n    \"\"\"\n    Middleware that checks if the model is currently scaling and\n    returns a 503 Service Unavailable response if it is.\n\n    This middleware applies to all HTTP requests and prevents\n    processing when the model is in a scaling state.\n    \"\"\"\n\n    def __init__(self, app: ASGIApp) -> None:\n        self.app = app\n\n    def __call__(self, scope: Scope, receive: Receive,\n                 send: Send) -> Awaitable[None]:\n        if scope[\"type\"] != \"http\":\n            return self.app(scope, receive, send)\n\n        # Check global scaling state\n        global _scaling_elastic_ep\n        if _scaling_elastic_ep:\n            # Return 503 Service Unavailable response\n            response = JSONResponse(content={\n                \"error\":\n                \"The model is currently scaling. Please try again later.\"\n            },\n                                    status_code=503)\n            return response(scope, receive, send)\n\n        return self.app(scope, receive, send)\n\n\ndef _extract_content_from_chunk(chunk_data: dict) -> str:\n    \"\"\"Extract content from a streaming response chunk.\"\"\"\n    try:\n        from vllm.entrypoints.openai.protocol import (\n            ChatCompletionStreamResponse, CompletionStreamResponse)\n\n        # Try using Completion types for type-safe parsing\n        if chunk_data.get('object') == 'chat.completion.chunk':\n            chat_response = ChatCompletionStreamResponse.model_validate(\n                chunk_data)\n            if chat_response.choices and chat_response.choices[0].delta.content:\n                return chat_response.choices[0].delta.content\n        elif chunk_data.get('object') == 'text_completion':\n            completion_response = CompletionStreamResponse.model_validate(\n                chunk_data)\n            if completion_response.choices and completion_response.choices[\n                    0].text:\n                return completion_response.choices[0].text\n    except pydantic.ValidationError:\n        # Fallback to manual parsing\n        if 'choices' in chunk_data and chunk_data['choices']:\n            choice = chunk_data['choices'][0]\n            if 'delta' in choice and choice['delta'].get('content'):\n                return choice['delta']['content']\n            elif choice.get('text'):\n                return choice['text']\n    return \"\"\n\n\nclass SSEDecoder:\n    \"\"\"Robust Server-Sent Events decoder for streaming responses.\"\"\"\n\n    def __init__(self):\n        self.buffer = \"\"\n        self.content_buffer = []\n\n    def decode_chunk(self, chunk: bytes) -> list[dict]:\n        \"\"\"Decode a chunk of SSE data and return parsed events.\"\"\"\n        import json\n\n        try:\n            chunk_str = chunk.decode('utf-8')\n        except UnicodeDecodeError:\n            # Skip malformed chunks\n            return []\n\n        self.buffer += chunk_str\n        events = []\n\n        # Process complete lines\n        while '\\n' in self.buffer:\n            line, self.buffer = self.buffer.split('\\n', 1)\n            line = line.rstrip('\\r')  # Handle CRLF\n\n            if line.startswith('data: '):\n                data_str = line[6:].strip()\n                if data_str == '[DONE]':\n                    events.append({'type': 'done'})\n                elif data_str:\n                    try:\n                        event_data = json.loads(data_str)\n                        events.append({'type': 'data', 'data': event_data})\n                    except json.JSONDecodeError:\n                        # Skip malformed JSON\n                        continue\n\n        return events\n\n    def extract_content(self, event_data: dict) -> str:\n        \"\"\"Extract content from event data.\"\"\"\n        return _extract_content_from_chunk(event_data)\n\n    def add_content(self, content: str) -> None:\n        \"\"\"Add content to the buffer.\"\"\"\n        if content:\n            self.content_buffer.append(content)\n\n    def get_complete_content(self) -> str:\n        \"\"\"Get the complete buffered content.\"\"\"\n        return ''.join(self.content_buffer)\n\n\ndef _log_streaming_response(response, response_body: list) -> None:\n    \"\"\"Log streaming response with robust SSE parsing.\"\"\"\n    from starlette.concurrency import iterate_in_threadpool\n\n    sse_decoder = SSEDecoder()\n    chunk_count = 0\n\n    def buffered_iterator():\n        nonlocal chunk_count\n\n        for chunk in response_body:\n            chunk_count += 1\n            yield chunk\n\n            # Parse SSE events from chunk\n            events = sse_decoder.decode_chunk(chunk)\n\n            for event in events:\n                if event['type'] == 'data':\n                    content = sse_decoder.extract_content(event['data'])\n                    sse_decoder.add_content(content)\n                elif event['type'] == 'done':\n                    # Log complete content when done\n                    full_content = sse_decoder.get_complete_content()\n                    if full_content:\n                        # Truncate if too long\n                        if len(full_content) > 2048:\n                            full_content = full_content[:2048] + \"\"\n                            \"...[truncated]\"\n                        logger.info(\n                            \"response_body={streaming_complete: \" \\\n                            \"content='%s', chunks=%d}\",\n                            full_content, chunk_count)\n                    else:\n                        logger.info(\n                            \"response_body={streaming_complete: \" \\\n                            \"no_content, chunks=%d}\",\n                            chunk_count)\n                    return\n\n    response.body_iterator = iterate_in_threadpool(buffered_iterator())\n    logger.info(\"response_body={streaming_started: chunks=%d}\",\n                len(response_body))\n\n\ndef _log_non_streaming_response(response_body: list) -> None:\n    \"\"\"Log non-streaming response.\"\"\"\n    try:\n        decoded_body = response_body[0].decode()\n        logger.info(\"response_body={%s}\", decoded_body)\n    except UnicodeDecodeError:\n        logger.info(\"response_body={<binary_data>}\")\n\n\ndef build_app(args: Namespace) -> FastAPI:\n    if args.disable_fastapi_docs:\n        app = FastAPI(openapi_url=None,\n                      docs_url=None,\n                      redoc_url=None,\n                      lifespan=lifespan)\n    else:\n        app = FastAPI(lifespan=lifespan)\n    app.include_router(router)\n    app.root_path = args.root_path\n\n    mount_metrics(app)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n\n    @app.exception_handler(HTTPException)\n    async def http_exception_handler(_: Request, exc: HTTPException):\n        err = ErrorResponse(message=exc.detail,\n                            type=HTTPStatus(exc.status_code).phrase,\n                            code=exc.status_code)\n        return JSONResponse(err.model_dump(), status_code=exc.status_code)\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(_: Request,\n                                           exc: RequestValidationError):\n        exc_str = str(exc)\n        errors_str = str(exc.errors())\n\n        if exc.errors() and errors_str and errors_str != exc_str:\n            message = f\"{exc_str} {errors_str}\"\n        else:\n            message = exc_str\n\n        err = ErrorResponse(message=message,\n                            type=HTTPStatus.BAD_REQUEST.phrase,\n                            code=HTTPStatus.BAD_REQUEST)\n        return JSONResponse(err.model_dump(),\n                            status_code=HTTPStatus.BAD_REQUEST)\n\n    # Ensure --api-key option from CLI takes precedence over VLLM_API_KEY\n    if tokens := [key for key in (args.api_key or [envs.VLLM_API_KEY]) if key]:\n        app.add_middleware(AuthenticationMiddleware, tokens=tokens)\n\n    if args.enable_request_id_headers:\n        app.add_middleware(XRequestIdMiddleware)\n\n    # Add scaling middleware to check for scaling state\n    app.add_middleware(ScalingMiddleware)\n\n    if envs.VLLM_DEBUG_LOG_API_SERVER_RESPONSE:\n        logger.warning(\"CAUTION: Enabling log response in the API Server. \"\n                       \"This can include sensitive information and should be \"\n                       \"avoided in production.\")\n\n        @app.middleware(\"http\")\n        async def log_response(request: Request, call_next):\n            response = await call_next(request)\n            response_body = [\n                section async for section in response.body_iterator\n            ]\n            response.body_iterator = iterate_in_threadpool(iter(response_body))\n            # Check if this is a streaming response by looking at content-type\n            content_type = response.headers.get(\"content-type\", \"\")\n            is_streaming = content_type == \"text/event-stream; charset=utf-8\"\n\n            # Log response body based on type\n            if not response_body:\n                logger.info(\"response_body={<empty>}\")\n            elif is_streaming:\n                _log_streaming_response(response, response_body)\n            else:\n                _log_non_streaming_response(response_body)\n            return response\n\n    for middleware in args.middleware:\n        module_path, object_name = middleware.rsplit(\".\", 1)\n        imported = getattr(importlib.import_module(module_path), object_name)\n        if inspect.isclass(imported):\n            app.add_middleware(imported)  # type: ignore[arg-type]\n        elif inspect.iscoroutinefunction(imported):\n            app.middleware(\"http\")(imported)\n        else:\n            raise ValueError(f\"Invalid middleware {middleware}. \"\n                             f\"Must be a function or a class.\")\n\n    return app\n\n\nasync def init_app_state(\n    engine_client: EngineClient,\n    vllm_config: VllmConfig,\n    state: State,\n    args: Namespace,\n) -> None:\n    if args.served_model_name is not None:\n        served_model_names = args.served_model_name\n    else:\n        served_model_names = [args.model]\n\n    if args.enable_log_requests:\n        request_logger = RequestLogger(max_log_len=args.max_log_len)\n    else:\n        request_logger = None\n\n    base_model_paths = [\n        BaseModelPath(name=name, model_path=args.model)\n        for name in served_model_names\n    ]\n\n    state.engine_client = engine_client\n    state.log_stats = not args.disable_log_stats\n    state.vllm_config = vllm_config\n    model_config = vllm_config.model_config\n\n    if envs.VLLM_USE_V1:\n        supported_tasks = await engine_client \\\n            .get_supported_tasks()  # type: ignore\n    else:\n        supported_tasks = model_config.supported_tasks\n\n    logger.info(\"Supported_tasks: %s\", supported_tasks)\n\n    resolved_chat_template = load_chat_template(args.chat_template)\n    if resolved_chat_template is not None:\n        # Get the tokenizer to check official template\n        tokenizer = await engine_client.get_tokenizer()\n\n        if isinstance(tokenizer, MistralTokenizer):\n            # The warning is logged in resolve_mistral_chat_template.\n            resolved_chat_template = resolve_mistral_chat_template(\n                chat_template=resolved_chat_template)\n        else:\n            hf_chat_template = resolve_hf_chat_template(\n                tokenizer=tokenizer,\n                chat_template=None,\n                tools=None,\n                model_config=vllm_config.model_config,\n            )\n\n            if hf_chat_template != resolved_chat_template:\n                logger.warning(\n                    \"Using supplied chat template: %s\\n\"\n                    \"It is different from official chat template '%s'. \"\n                    \"This discrepancy may lead to performance degradation.\",\n                    resolved_chat_template, args.model)\n\n    if args.tool_server == \"demo\":\n        tool_server: Optional[ToolServer] = DemoToolServer()\n    elif args.tool_server:\n        tool_server = MCPToolServer()\n        await tool_server.add_tool_server(args.tool_server)\n    else:\n        tool_server = None\n\n    # Merge default_mm_loras into the static lora_modules\n    default_mm_loras = (vllm_config.lora_config.default_mm_loras\n                        if vllm_config.lora_config is not None else {})\n\n    lora_modules = args.lora_modules\n    if default_mm_loras:\n        default_mm_lora_paths = [\n            LoRAModulePath(\n                name=modality,\n                path=lora_path,\n            ) for modality, lora_path in default_mm_loras.items()\n        ]\n        if args.lora_modules is None:\n            lora_modules = default_mm_lora_paths\n        else:\n            lora_modules += default_mm_lora_paths\n\n    state.openai_serving_models = OpenAIServingModels(\n        engine_client=engine_client,\n        model_config=model_config,\n        base_model_paths=base_model_paths,\n        lora_modules=lora_modules,\n    )\n    await state.openai_serving_models.init_static_loras()\n    state.openai_serving_responses = OpenAIServingResponses(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n        return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n        enable_auto_tools=args.enable_auto_tool_choice,\n        tool_parser=args.tool_call_parser,\n        tool_server=tool_server,\n        reasoning_parser=args.reasoning_parser,\n        enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n        enable_force_include_usage=args.enable_force_include_usage,\n    ) if \"generate\" in supported_tasks else None\n    state.openai_serving_chat = OpenAIServingChat(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        args.response_role,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n        return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n        enable_auto_tools=args.enable_auto_tool_choice,\n        exclude_tools_when_tool_choice_none=args.\n        exclude_tools_when_tool_choice_none,\n        tool_parser=args.tool_call_parser,\n        reasoning_parser=args.reasoning_parser,\n        enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n        enable_force_include_usage=args.enable_force_include_usage,\n    ) if \"generate\" in supported_tasks else None\n    state.openai_serving_completion = OpenAIServingCompletion(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n        enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n        enable_force_include_usage=args.enable_force_include_usage,\n    ) if \"generate\" in supported_tasks else None\n    state.openai_serving_pooling = OpenAIServingPooling(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    ) if \"encode\" in supported_tasks else None\n    state.openai_serving_embedding = OpenAIServingEmbedding(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    ) if \"embed\" in supported_tasks else None\n    state.openai_serving_classification = ServingClassification(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n    ) if \"classify\" in supported_tasks else None\n\n    enable_serving_reranking = (\"classify\" in supported_tasks and getattr(\n        model_config.hf_config, \"num_labels\", 0) == 1)\n    state.openai_serving_scores = ServingScores(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n    ) if (\"embed\" in supported_tasks or enable_serving_reranking) else None\n\n    state.openai_serving_tokenization = OpenAIServingTokenization(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    )\n    state.openai_serving_transcription = OpenAIServingTranscription(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n    ) if \"transcription\" in supported_tasks else None\n    state.openai_serving_translation = OpenAIServingTranslation(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n    ) if \"transcription\" in supported_tasks else None\n\n    state.enable_server_load_tracking = args.enable_server_load_tracking\n    state.server_load_metrics = 0\n\n\ndef create_server_socket(addr: tuple[str, int]) -> socket.socket:\n    family = socket.AF_INET\n    if is_valid_ipv6_address(addr[0]):\n        family = socket.AF_INET6\n\n    sock = socket.socket(family=family, type=socket.SOCK_STREAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n    sock.bind(addr)\n\n    return sock\n\n\ndef validate_api_server_args(args):\n    valid_tool_parses = ToolParserManager.tool_parsers.keys()\n    if args.enable_auto_tool_choice \\\n            and args.tool_call_parser not in valid_tool_parses:\n        raise KeyError(f\"invalid tool call parser: {args.tool_call_parser} \"\n                       f\"(chose from {{ {','.join(valid_tool_parses)} }})\")\n\n    valid_reasoning_parses = ReasoningParserManager.reasoning_parsers.keys()\n    if args.reasoning_parser \\\n        and args.reasoning_parser not in valid_reasoning_parses:\n        raise KeyError(\n            f\"invalid reasoning parser: {args.reasoning_parser} \"\n            f\"(chose from {{ {','.join(valid_reasoning_parses)} }})\")\n\n\ndef setup_server(args):\n    \"\"\"Validate API server args, set up signal handler, create socket\n    ready to serve.\"\"\"\n\n    logger.info(\"vLLM API server version %s\", VLLM_VERSION)\n    log_non_default_args(args)\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    validate_api_server_args(args)\n\n    # workaround to make sure that we bind the port before the engine is set up.\n    # This avoids race conditions with ray.\n    # see https://github.com/vllm-project/vllm/issues/8204\n    sock_addr = (args.host or \"\", args.port)\n    sock = create_server_socket(sock_addr)\n\n    # workaround to avoid footguns where uvicorn drops requests with too\n    # many concurrent requests active\n    set_ulimit()\n\n    def signal_handler(*_) -> None:\n        # Interrupt server on sigterm while initializing\n        raise KeyboardInterrupt(\"terminated\")\n\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    addr, port = sock_addr\n    is_ssl = args.ssl_keyfile and args.ssl_certfile\n    host_part = f\"[{addr}]\" if is_valid_ipv6_address(\n        addr) else addr or \"0.0.0.0\"\n    listen_address = f\"http{'s' if is_ssl else ''}://{host_part}:{port}\"\n\n    return listen_address, sock\n\n\nasync def run_server(args, **uvicorn_kwargs) -> None:\n    \"\"\"Run a single-worker API server.\"\"\"\n\n    # Add process-specific prefix to stdout and stderr.\n    decorate_logs(\"APIServer\")\n\n    listen_address, sock = setup_server(args)\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\n\nasync def run_server_worker(listen_address,\n                            sock,\n                            args,\n                            client_config=None,\n                            **uvicorn_kwargs) -> None:\n    \"\"\"Run a single API server worker.\"\"\"\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    server_index = client_config.get(\"client_index\", 0) if client_config else 0\n\n    # Load logging config for uvicorn if specified\n    log_config = load_log_config(args.log_config_file)\n    if log_config is not None:\n        uvicorn_kwargs['log_config'] = log_config\n\n    async with build_async_engine_client(\n            args,\n            client_config=client_config,\n    ) as engine_client:\n        maybe_register_tokenizer_info_endpoint(args)\n        app = build_app(args)\n\n        vllm_config = await engine_client.get_vllm_config()\n        await init_app_state(engine_client, vllm_config, app.state, args)\n\n        logger.info(\"Starting vLLM API server %d on %s\", server_index,\n                    listen_address)\n        shutdown_task = await serve_http(\n            app,\n            sock=sock,\n            enable_ssl_refresh=args.enable_ssl_refresh,\n            host=args.host,\n            port=args.port,\n            log_level=args.uvicorn_log_level,\n            # NOTE: When the 'disable_uvicorn_access_log' value is True,\n            # no access log will be output.\n            access_log=not args.disable_uvicorn_access_log,\n            timeout_keep_alive=envs.VLLM_HTTP_TIMEOUT_KEEP_ALIVE,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            ssl_ca_certs=args.ssl_ca_certs,\n            ssl_cert_reqs=args.ssl_cert_reqs,\n            **uvicorn_kwargs,\n        )\n\n    # NB: Await server shutdown only after the backend context is exited\n    try:\n        await shutdown_task\n    finally:\n        sock.close()\n\n\nif __name__ == \"__main__\":\n    # NOTE(simon):\n    # This section should be in sync with vllm/entrypoints/cli/main.py for CLI\n    # entrypoints.\n    cli_env_setup()\n    parser = FlexibleArgumentParser(\n        description=\"vLLM OpenAI-Compatible RESTful API server.\")\n    parser = make_arg_parser(parser)\n    args = parser.parse_args()\n    validate_parsed_serve_args(args)\n\n    uvloop.run(run_server(args))\n", 1896], "/usr/lib/python3.12/multiprocessing/util.py": ["#\n# Module providing various facilities to other parts of the package\n#\n# multiprocessing/util.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\nimport os\nimport itertools\nimport sys\nimport weakref\nimport atexit\nimport threading        # we want threading to install it's\n                        # cleanup function before multiprocessing does\nfrom subprocess import _args_from_interpreter_flags\n\nfrom . import process\n\n__all__ = [\n    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',\n    'log_to_stderr', 'get_temp_dir', 'register_after_fork',\n    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',\n    'close_all_fds_except', 'SUBDEBUG', 'SUBWARNING',\n    ]\n\n#\n# Logging\n#\n\nNOTSET = 0\nSUBDEBUG = 5\nDEBUG = 10\nINFO = 20\nSUBWARNING = 25\n\nLOGGER_NAME = 'multiprocessing'\nDEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'\n\n_logger = None\n_log_to_stderr = False\n\ndef sub_debug(msg, *args):\n    if _logger:\n        _logger.log(SUBDEBUG, msg, *args, stacklevel=2)\n\ndef debug(msg, *args):\n    if _logger:\n        _logger.log(DEBUG, msg, *args, stacklevel=2)\n\ndef info(msg, *args):\n    if _logger:\n        _logger.log(INFO, msg, *args, stacklevel=2)\n\ndef sub_warning(msg, *args):\n    if _logger:\n        _logger.log(SUBWARNING, msg, *args, stacklevel=2)\n\ndef get_logger():\n    '''\n    Returns logger used by multiprocessing\n    '''\n    global _logger\n    import logging\n\n    logging._acquireLock()\n    try:\n        if not _logger:\n\n            _logger = logging.getLogger(LOGGER_NAME)\n            _logger.propagate = 0\n\n            # XXX multiprocessing should cleanup before logging\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(_exit_function)\n                atexit.register(_exit_function)\n            else:\n                atexit._exithandlers.remove((_exit_function, (), {}))\n                atexit._exithandlers.append((_exit_function, (), {}))\n\n    finally:\n        logging._releaseLock()\n\n    return _logger\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    global _log_to_stderr\n    import logging\n\n    logger = get_logger()\n    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if level:\n        logger.setLevel(level)\n    _log_to_stderr = True\n    return _logger\n\n\n# Abstract socket support\n\ndef _platform_supports_abstract_sockets():\n    if sys.platform == \"linux\":\n        return True\n    if hasattr(sys, 'getandroidapilevel'):\n        return True\n    return False\n\n\ndef is_abstract_socket_namespace(address):\n    if not address:\n        return False\n    if isinstance(address, bytes):\n        return address[0] == 0\n    elif isinstance(address, str):\n        return address[0] == \"\\0\"\n    raise TypeError(f'address type of {address!r} unrecognized')\n\n\nabstract_sockets_supported = _platform_supports_abstract_sockets()\n\n#\n# Function returning a temp directory which will be removed on exit\n#\n\ndef _remove_temp_dir(rmtree, tempdir):\n    def onerror(func, path, err_info):\n        if not issubclass(err_info[0], FileNotFoundError):\n            raise\n    rmtree(tempdir, onerror=onerror)\n\n    current_process = process.current_process()\n    # current_process() can be None if the finalizer is called\n    # late during Python finalization\n    if current_process is not None:\n        current_process._config['tempdir'] = None\n\ndef get_temp_dir():\n    # get name of a temp directory which will be automatically cleaned up\n    tempdir = process.current_process()._config.get('tempdir')\n    if tempdir is None:\n        import shutil, tempfile\n        tempdir = tempfile.mkdtemp(prefix='pymp-')\n        info('created temp directory %s', tempdir)\n        # keep a strong reference to shutil.rmtree(), since the finalizer\n        # can be called late during Python shutdown\n        Finalize(None, _remove_temp_dir, args=(shutil.rmtree, tempdir),\n                 exitpriority=-100)\n        process.current_process()._config['tempdir'] = tempdir\n    return tempdir\n\n#\n# Support for reinitialization of objects when bootstrapping a child process\n#\n\n_afterfork_registry = weakref.WeakValueDictionary()\n_afterfork_counter = itertools.count()\n\ndef _run_after_forkers():\n    items = list(_afterfork_registry.items())\n    items.sort()\n    for (index, ident, func), obj in items:\n        try:\n            func(obj)\n        except Exception as e:\n            info('after forker raised exception %s', e)\n\ndef register_after_fork(obj, func):\n    _afterfork_registry[(next(_afterfork_counter), id(obj), func)] = obj\n\n#\n# Finalization using weakrefs\n#\n\n_finalizer_registry = {}\n_finalizer_counter = itertools.count()\n\n\nclass Finalize(object):\n    '''\n    Class which supports object finalization using weakrefs\n    '''\n    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):\n        if (exitpriority is not None) and not isinstance(exitpriority,int):\n            raise TypeError(\n                \"Exitpriority ({0!r}) must be None or int, not {1!s}\".format(\n                    exitpriority, type(exitpriority)))\n\n        if obj is not None:\n            self._weakref = weakref.ref(obj, self)\n        elif exitpriority is None:\n            raise ValueError(\"Without object, exitpriority cannot be None\")\n\n        self._callback = callback\n        self._args = args\n        self._kwargs = kwargs or {}\n        self._key = (exitpriority, next(_finalizer_counter))\n        self._pid = os.getpid()\n\n        _finalizer_registry[self._key] = self\n\n    def __call__(self, wr=None,\n                 # Need to bind these locally because the globals can have\n                 # been cleared at shutdown\n                 _finalizer_registry=_finalizer_registry,\n                 sub_debug=sub_debug, getpid=os.getpid):\n        '''\n        Run the callback unless it has already been called or cancelled\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            sub_debug('finalizer no longer registered')\n        else:\n            if self._pid != getpid():\n                sub_debug('finalizer ignored because different process')\n                res = None\n            else:\n                sub_debug('finalizer calling %s with args %s and kwargs %s',\n                          self._callback, self._args, self._kwargs)\n                res = self._callback(*self._args, **self._kwargs)\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n            return res\n\n    def cancel(self):\n        '''\n        Cancel finalization of the object\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            pass\n        else:\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n\n    def still_active(self):\n        '''\n        Return whether this finalizer is still waiting to invoke callback\n        '''\n        return self._key in _finalizer_registry\n\n    def __repr__(self):\n        try:\n            obj = self._weakref()\n        except (AttributeError, TypeError):\n            obj = None\n\n        if obj is None:\n            return '<%s object, dead>' % self.__class__.__name__\n\n        x = '<%s object, callback=%s' % (\n                self.__class__.__name__,\n                getattr(self._callback, '__name__', self._callback))\n        if self._args:\n            x += ', args=' + str(self._args)\n        if self._kwargs:\n            x += ', kwargs=' + str(self._kwargs)\n        if self._key[0] is not None:\n            x += ', exitpriority=' + str(self._key[0])\n        return x + '>'\n\n\ndef _run_finalizers(minpriority=None):\n    '''\n    Run all finalizers whose exit priority is not None and at least minpriority\n\n    Finalizers with highest priority are called first; finalizers with\n    the same priority will be called in reverse order of creation.\n    '''\n    if _finalizer_registry is None:\n        # This function may be called after this module's globals are\n        # destroyed.  See the _exit_function function in this module for more\n        # notes.\n        return\n\n    if minpriority is None:\n        f = lambda p : p[0] is not None\n    else:\n        f = lambda p : p[0] is not None and p[0] >= minpriority\n\n    # Careful: _finalizer_registry may be mutated while this function\n    # is running (either by a GC run or by another thread).\n\n    # list(_finalizer_registry) should be atomic, while\n    # list(_finalizer_registry.items()) is not.\n    keys = [key for key in list(_finalizer_registry) if f(key)]\n    keys.sort(reverse=True)\n\n    for key in keys:\n        finalizer = _finalizer_registry.get(key)\n        # key may have been removed from the registry\n        if finalizer is not None:\n            sub_debug('calling %s', finalizer)\n            try:\n                finalizer()\n            except Exception:\n                import traceback\n                traceback.print_exc()\n\n    if minpriority is None:\n        _finalizer_registry.clear()\n\n#\n# Clean up on exit\n#\n\ndef is_exiting():\n    '''\n    Returns true if the process is shutting down\n    '''\n    return _exiting or _exiting is None\n\n_exiting = False\n\ndef _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n                   active_children=process.active_children,\n                   current_process=process.current_process):\n    # We hold on to references to functions in the arglist due to the\n    # situation described below, where this function is called after this\n    # module's globals are destroyed.\n\n    global _exiting\n\n    if not _exiting:\n        _exiting = True\n\n        info('process shutting down')\n        debug('running all \"atexit\" finalizers with priority >= 0')\n        _run_finalizers(0)\n\n        if current_process() is not None:\n            # We check if the current process is None here because if\n            # it's None, any call to ``active_children()`` will raise\n            # an AttributeError (active_children winds up trying to\n            # get attributes from util._current_process).  One\n            # situation where this can happen is if someone has\n            # manipulated sys.modules, causing this module to be\n            # garbage collected.  The destructor for the module type\n            # then replaces all values in the module dict with None.\n            # For instance, after setuptools runs a test it replaces\n            # sys.modules with a copy created earlier.  See issues\n            # #9775 and #15881.  Also related: #4106, #9205, and\n            # #9207.\n\n            for p in active_children():\n                if p.daemon:\n                    info('calling terminate() for daemon %s', p.name)\n                    p._popen.terminate()\n\n            for p in active_children():\n                info('calling join() for process %s', p.name)\n                p.join()\n\n        debug('running the remaining \"atexit\" finalizers')\n        _run_finalizers()\n\natexit.register(_exit_function)\n\n#\n# Some fork aware types\n#\n\nclass ForkAwareThreadLock(object):\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n        register_after_fork(self, ForkAwareThreadLock._at_fork_reinit)\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n\nclass ForkAwareLocal(threading.local):\n    def __init__(self):\n        register_after_fork(self, lambda obj : obj.__dict__.clear())\n    def __reduce__(self):\n        return type(self), ()\n\n#\n# Close fds except those specified\n#\n\ntry:\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\nexcept Exception:\n    MAXFD = 256\n\ndef close_all_fds_except(fds):\n    fds = list(fds) + [-1, MAXFD]\n    fds.sort()\n    assert fds[-1] == MAXFD, 'fd too large'\n    for i in range(len(fds) - 1):\n        os.closerange(fds[i]+1, fds[i+1])\n#\n# Close sys.stdin and replace stdin with os.devnull\n#\n\ndef _close_stdin():\n    if sys.stdin is None:\n        return\n\n    try:\n        sys.stdin.close()\n    except (OSError, ValueError):\n        pass\n\n    try:\n        fd = os.open(os.devnull, os.O_RDONLY)\n        try:\n            sys.stdin = open(fd, encoding=\"utf-8\", closefd=False)\n        except:\n            os.close(fd)\n            raise\n    except (OSError, ValueError):\n        pass\n\n#\n# Flush standard streams, if any\n#\n\ndef _flush_std_streams():\n    try:\n        sys.stdout.flush()\n    except (AttributeError, ValueError):\n        pass\n    try:\n        sys.stderr.flush()\n    except (AttributeError, ValueError):\n        pass\n\n#\n# Start a program with only specified fds kept open\n#\n\ndef spawnv_passfds(path, args, passfds):\n    import _posixsubprocess\n    import subprocess\n    passfds = tuple(sorted(map(int, passfds)))\n    errpipe_read, errpipe_write = os.pipe()\n    try:\n        return _posixsubprocess.fork_exec(\n            args, [path], True, passfds, None, None,\n            -1, -1, -1, -1, -1, -1, errpipe_read, errpipe_write,\n            False, False, -1, None, None, None, -1, None,\n            subprocess._USE_VFORK)\n    finally:\n        os.close(errpipe_read)\n        os.close(errpipe_write)\n\n\ndef close_fds(*fds):\n    \"\"\"Close each file descriptor given as an argument\"\"\"\n    for fd in fds:\n        os.close(fd)\n\n\ndef _cleanup_tests():\n    \"\"\"Cleanup multiprocessing resources when multiprocessing tests\n    completed.\"\"\"\n\n    from test import support\n\n    # cleanup multiprocessing\n    process._cleanup()\n\n    # Stop the ForkServer process if it's running\n    from multiprocessing import forkserver\n    forkserver._forkserver._stop()\n\n    # Stop the ResourceTracker process if it's running\n    from multiprocessing import resource_tracker\n    resource_tracker._resource_tracker._stop()\n\n    # bpo-37421: Explicitly call _run_finalizers() to remove immediately\n    # temporary directories created by multiprocessing.util.get_temp_dir().\n    _run_finalizers()\n    support.gc_collect()\n\n    support.reap_children()\n", 494]}, "functions": {"_call_with_frames_removed (<frozen importlib._bootstrap>:480)": ["<frozen importlib._bootstrap>", 480], "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)": ["<frozen importlib._bootstrap>", 1304], "_find_and_load (<frozen importlib._bootstrap>:1349)": ["<frozen importlib._bootstrap>", 1349], "_get_module_details (<frozen runpy>:105)": ["<frozen runpy>", 105], "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)": ["<frozen importlib._bootstrap_external>", 993], "_load_unlocked (<frozen importlib._bootstrap>:911)": ["<frozen importlib._bootstrap>", 911], "<module> (/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py:1)": ["/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", 1], "_run_code (<frozen runpy>:65)": ["<frozen runpy>", 65], "_run_module_code (<frozen runpy>:91)": ["<frozen runpy>", 91], "run_module (<frozen runpy>:201)": ["<frozen runpy>", 201], "Finalize.__call__ (/usr/lib/python3.12/multiprocessing/util.py:208)": ["/usr/lib/python3.12/multiprocessing/util.py", 208], "_run_finalizers (/usr/lib/python3.12/multiprocessing/util.py:271)": ["/usr/lib/python3.12/multiprocessing/util.py", 271], "_exit_function (/usr/lib/python3.12/multiprocessing/util.py:323)": ["/usr/lib/python3.12/multiprocessing/util.py", 323]}}}